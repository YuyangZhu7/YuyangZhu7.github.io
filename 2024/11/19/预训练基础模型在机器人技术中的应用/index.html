<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="预训练基础模型在机器人技术中的应用 传统深度学习模型:特定任务量身定制的小数据集上进行训练的  这限制了它们在不同应用中的适应性。  互联网规模数据上预训练的基础模型  优越的泛化能力，可以找到训练数据中不存在的问题的零样本解决方案。  基础模型可能有潜力增强机器人自主堆栈的各个组成部分，从感知到决策和控制。 大型语言模型可以生成代码或提供常识性推理 视觉语言模型支持开放词汇表视觉识别。 重大的开">
<meta property="og:type" content="article">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://example.com/2024/11/19/%E9%A2%84%E8%AE%AD%E7%BB%83%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B%E5%9C%A8%E6%9C%BA%E5%99%A8%E4%BA%BA%E6%8A%80%E6%9C%AF%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="预训练基础模型在机器人技术中的应用 传统深度学习模型:特定任务量身定制的小数据集上进行训练的  这限制了它们在不同应用中的适应性。  互联网规模数据上预训练的基础模型  优越的泛化能力，可以找到训练数据中不存在的问题的零样本解决方案。  基础模型可能有潜力增强机器人自主堆栈的各个组成部分，从感知到决策和控制。 大型语言模型可以生成代码或提供常识性推理 视觉语言模型支持开放词汇表视觉识别。 重大的开">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="c:\Users\%E6%9C%B1%E5%AE%87%E9%98%B3\AppData\Roaming\Typora\typora-user-images\1720169790993.png">
<meta property="article:published_time" content="2024-11-19T13:19:40.288Z">
<meta property="article:modified_time" content="2024-11-19T13:19:40.375Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="c:\Users\%E6%9C%B1%E5%AE%87%E9%98%B3\AppData\Roaming\Typora\typora-user-images\1720169790993.png">

<link rel="canonical" href="http://example.com/2024/11/19/%E9%A2%84%E8%AE%AD%E7%BB%83%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B%E5%9C%A8%E6%9C%BA%E5%99%A8%E4%BA%BA%E6%8A%80%E6%9C%AF%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title> | Hexo</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Hexo</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2024/11/19/%E9%A2%84%E8%AE%AD%E7%BB%83%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B%E5%9C%A8%E6%9C%BA%E5%99%A8%E4%BA%BA%E6%8A%80%E6%9C%AF%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2024-11-19 21:19:40" itemprop="dateCreated datePublished" datetime="2024-11-19T21:19:40+08:00">2024-11-19</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1>预训练基础模型在机器人技术中的应用</h1>
<p>传统深度学习模型:特定任务量身定制的小数据集上进行训练的</p>
<blockquote>
<p>这限制了它们在不同应用中的适应性。</p>
</blockquote>
<p>互联网规模数据上预训练的基础模型</p>
<blockquote>
<p>优越的泛化能力，可以找到训练数据中不存在的问题的零样本解决方案。</p>
</blockquote>
<p>基础模型可能有潜力增强机器人自主堆栈的各个组成部分，从感知到决策和控制。</p>
<p>大型语言模型可以生成代码或提供常识性推理</p>
<p>视觉语言模型支持开放词汇表视觉识别。</p>
<p>重大的开放研究挑战:机器人相关训练数据的稀缺性、安全保证和不确定性量化以及实时执行方面。</p>
<p>基础模型：在广泛的互联网规模数据上进行预训练的，并且可以进行微调以适应广泛的下游任务。基础模型在视觉和语言处理方面取得了重大突破</p>
<blockquote>
<p>例如BERT[1]、GPT-3[2]、GPT-4[3]、CLIP[4]、DALL-E[5]、PaLM-E[6]等。预训练的大型语言模型(LLMs)、大型视觉语言模型(VLMs)、大型音频语言模型(alm)和大型视觉导航模型(VNMs)可用于改进机器人设置中的各种任务。将基础模型集成到机器人中是一个快速发展的领域，机器人社区最近开始探索利用机器人领域中的这些大型模型进行感知、预测、计划和控制的方法。</p>
</blockquote>
<p>传统深度学习模型通常是在为不同任务收集的有限数据集上进行训练的[7]。</p>
<p>基础模型是在广泛和多样化的数据上进行预训练的，这在其他领域(如自然语言处理、计算机视觉和医疗保健[8])中得到了证明，可以显著扩展适应性、泛化能力和整体性能。</p>
<p>最终，基础模型可能会在机器人技术中产生同样的好处。与特定任务相比，基础模型的知识转移可以减少训练时间和计算资源。我们承诺进一步加强和更新这项工作，以确保其质量和相关模式。与机器人技术特别相关的是，多模态基础模型可以将从各种传感器收集的多模态异构数据融合和对齐为机器人理解和推理所需的紧凑的同构表示[9]。</p>
<p>这些学习到的表示有可能用于自主性堆栈的任何部分，包括感知、决策和控制。此外，基础模型提供了零样本能力，这是指人工智能系统在没有事先示例或特定任务专用训练数据的情况下执行任务的能力。这将使机器人能够将所学的知识推广到新的案例中，增强机器人在非结构化环境中的适应性和灵活性。</p>
<p>将基础模型集成到机器人系统中可以通过增强机器人感知环境和与环境交互的能力来实现上下文感知机器人系统。例如，在感知领域，已经发现大型视觉语言模型(VLMs)通过学习视觉和文本数据之间的关联来提供跨模态理解，帮助完成诸如零镜头图像分类、零镜头物体检测[10]和3D分类[11]等任务。</p>
<p>另一个例子是，3D世界中的语言基础(将vlm的上下文理解与三维(3D)现实世界对齐)可以通过将单词与3D环境中的特定对象、位置或动作相关联来增强机器人的空间意识。</p>
<p>在决策或规划领域，已经发现llm和vlm可以帮助机器人进行高级规划的任务规范[13]。机器人可以通过在操作、导航和交互中利用语言线索来执行更复杂的任务。例如，对于模仿学习[14]和强化学习[15]等机器人策略学习技术，基础模型似乎提供了提高数据效率和增强上下文理解的可能性。特别是，语言驱动的奖励可以通过提供形状奖励来指导强化学习代理[16]。</p>
<p>此外，研究人员还利用语言模型为政策学习技术提供反馈[17]。一些研究表明，VLM模型的视觉问答(VQA)能力可以在机器人用例中得到利用。例如，研究人员已经使用vlm来回答与视觉内容相关的问题，以帮助机器人完成任务[18]。</p>
<p>此外，研究人员已经声明利用vlm来帮助数据注释，通过为视觉内容生成描述性标签[19]。</p>
<p>尽管基础模型在视觉和语言处理方面具有变革性的能力，但基础模型在现实世界机器人任务中的泛化和微调仍然具有挑战性。这些挑战包括:</p>
<p>1)数据稀缺:如何获得机器人操作、运动、导航和其他机器人任务的互联网级数据，以及如何使用这些数据进行自监督训练;</p>
<p>2)高可变性:如何处理物理环境、物理机器人平台和潜在机器人任务的巨大多样性，同时保持基础模型所需的通用性;</p>
<p>3)不确定性量化:如何处理(i)实例级的不确定性，如语言歧义或LLM幻觉;(ii)分布层面的不确定性;</p>
<p>4)安全评估:如何严格测试基于基础模型的机器人系统的安全性(i)在部署之前，(ii)在其整个生命周期中更新模型，以及(iii)机器人在目标环境中运行时。</p>
<p>5)实时性:如何解决一些基础模型的高推理时间阻碍其在机器人上部署的问题，如何将基础模型的推理速度加快到在线决策所需的速度。</p>
<p>在这项调查中，我们研究了现有的关于基础模型在机器人中的应用的文献。我们研究了当前的方法和应用，提出了当前的挑战，提出了未来研究的方向，以解决这些挑战，并确定了将基础模型集成到机器人自治中所暴露的潜在风险。另一个关于机器人基础模型的调查与我们在arXiv上的调查同时出现[20]。与那篇论文相比，我们的论文强调了未来的挑战和机遇，包括安全性和风险，我们的论文更强调了该领域现有论文在应用、算法和架构方面的比较。与一些现有的专注于特定上下文指令的调查(如提示[21]、视觉变形[22]或决策[13]、[23])相比，我们提供了一个更广阔的视角，将基础模型中的不同研究线索联系起来，围绕它们与机器人的相关性和应用进行组织。相反，我们的范围比论文[24]窄得多，论文探讨了基础模型在许多学科中的广泛应用，机器人就是其中之一。我们希望这篇论文能够明确研究领域的最新进展和存在的不足，并指出该研究领域未来面临的机遇和挑战。最终，我们的目标是为机器人研究人员提供一个了解这个令人兴奋的新领域的资源。</p>
<p>我们将本次调查的范围限制为以下类别之一的论文:</p>
<p>1)背景论文:与机器人技术没有明确联系，但仍然是理解基础模型所必需的论文。这些论文将在调查报告的背景部分(第二部分)进行讨论。</p>
<p>2)机器人论文:以即插即用的方式将基础模型集成到机器人系统中的论文，针对机器人系统调整或微调基础模型的论文，或者构建新的机器人特定基础模型的论文。</p>
<p>3)机器人相关论文:介绍应用于机器人相关领域的方法或技术(例如，计算机视觉，嵌入式人工智能)，并有明确的未来机器人应用路径的论文。</p>
<p>本调查的组织如下:</p>
<p>在第二节中，我们介绍了基础模型，包括llm、视觉转换器、vlm、具体化多模态语言模型和视觉生成模型。此外，在本节的最后一部分，我们讨论了用于训练基础模型的不同训练方法。</p>
<p>在第三节中，我们介绍了如何将基础模型集成到机器人决策的不同任务中。首先，我们讨论了使用语言条件模仿学习和语言辅助强化学习的机器人策略学习。然后，我们讨论了如何使用基础模型来设计可用于规划目的的语言条件值函数。其次，介绍了机器人任务规范和基于基础模型的任务规划代码生成。</p>
<p>在第四节中，我们研究了机器人中的各种感知任务，这些任务有可能通过使用基础模型来增强。这些任务包括语义分割、3D场景表示、零镜头3D分类、可视性预测和动态预测。</p>
<p>在第五节中，我们介绍了关于具身AI代理，通才AI代理以及为具身AI研究开发的模拟器和基准的论文。</p>
<p>在第六节中，我们通过讨论在机器人系统中使用基础模型的不同挑战并提出未来研究的潜在途径来结束调查。</p>
<p>最后，我们在第七节作结束语</p>
<h2>基础模型背景</h2>
<p>基础模型有数十亿个参数，并在大量互联网规模的数据集上进行预训练。这种规模和复杂性的训练模式涉及大量成本。</p>
<p>获取、处理和管理数据的成本很高。训练过程需要大量的计算资源，需要专门的硬件，如gpu或tpu，以及需要财政资源的模型训练的软件和基础设施。此外，训练基础模型需要耗费大量时间，这可能会导致更高的成本。因此，这些模型经常被用作即插即用模块(它指的是将基础模型集成到各种应用程序中，而不需要进行广泛的定制)。表1提供了常用基础模型的详细信息。在本节的其余部分，我们将介绍llm、视觉转换器、vlm、具体化的多模态语言模型和视觉生成模型。在本节的最后一部分，我们将介绍用于训练基础模型的不同训练方法。</p>
<img src="C:\Users\朱宇阳\AppData\Roaming\Typora\typora-user-images\1720169790993.png" alt="1720169790993" style="zoom:150%;">
<p>A.术语和数学基础</p>
<p>在本节中，我们首先介绍基础模型背景下的常用术语，并描述各种类型基础模型的基本数学细节和训练实践。</p>
<p>Tokenization 是自然语言处理（NLP）中的一个关键步骤，它涉及将文本（字符序列）分割成更小的单元，这些单元被称为“tokens”。这个过程对于机器学习模型，特别是那些处理文本数据的模型（如大型语言模型LLM）来说至关重要，因为它帮助模型理解和处理文本数据。下面是对该过程的详细解读：</p>
<h3>Tokenization 过程</h3>
<ol>
<li>
<p><strong>分割文本</strong>：给定一个字符序列（即文本），tokenization 的第一步是将这个序列分割成更小的单元。这些单元的具体形式取决于所采用的 tokenization 策略。</p>
</li>
<li>
<p><strong>Token 种类</strong>：</p>
<ul>
<li><strong>字符</strong>：最简单的形式，每个字符都被视为一个 token。</li>
<li><strong>词片段</strong>：某些情况下，单词可能被分割成更小的片段（如子词或词素），这有助于处理罕见词和拼写错误。</li>
<li><strong>完整单词</strong>：更常见的做法是将完整的单词作为 token。</li>
<li><strong>句子片段</strong>：在某些情况下，句子的一部分或整个句子也可以作为 token。</li>
</ul>
</li>
<li>
<p><strong>表示方法</strong>：分割后的 tokens 通常以 1-hot 向量的形式表示，其中向量的维度等于总词汇表的大小。每个 token 在其对应的维度上为 1，其余为 0。然而，这种方法在词汇表很大时会导致向量非常稀疏，计算效率低下。</p>
</li>
<li>
<p><strong>嵌入（Embedding）</strong>：为了克服 1-hot 向量的缺点，tokens 被映射到低维实数向量中，这个过程称为嵌入（embedding）。这通常通过一个学习得到的嵌入矩阵来实现，该矩阵在训练过程中被优化以捕捉 tokens 之间的语义关系。</p>
</li>
</ol>
<h3>LLM 和 GPT-3 的应用</h3>
<ul>
<li>
<p><strong>LLM（大型语言模型）</strong>：这些模型以嵌入向量的序列作为原始输入，并产生另一个嵌入向量的序列作为原始输出。这些输出向量随后被映射回 tokens，并最终转换回文本形式。</p>
</li>
<li>
<p><strong>GPT-3</strong>：作为 LLM 的一个例子，GPT-3 拥有一个包含 50,257 个不同 tokens 的词汇表，以及一个 12,288 维的嵌入空间。这意味着每个 token 都被表示为一个 12,288 维的实数向量，这使得模型能够捕捉复杂的语义和上下文信息。GPT-3 的这种大规模和复杂性使其能够在各种 NLP 任务中表现出色，包括文本生成、问答、翻译等。</p>
</li>
</ul>
<h3>总结</h3>
<p>Tokenization 是 NLP 中一个基础且重要的步骤，它通过将文本分割成更小的单元（tokens）并为这些单元提供有效的表示（如嵌入向量），为机器学习模型处理文本数据提供了基础。在大型语言模型（如 GPT-3）中，tokenization 和嵌入技术的结合使得模型能够理解和生成复杂的自然语言文本。</p>
<p>在理解您提出的问题时，我们关注的是标记解码过程中如何从低维实值嵌入向量转换为高维1-hot向量，并如何利用这些权重在文本生成中引入随机性，特别是在GPT-3等大规模语言模型（LLM）中的应用。以下是对这一过程的详细解释：</p>
<h3>1. 标记解码过程</h3>
<p>在自然语言处理（NLP）和深度学习领域，词嵌入（Word Embedding）是一种将词汇表中的单词或标记映射到低维连续向量空间的技术。这些低维向量能够捕捉单词之间的语义关系，并作为模型的输入进行进一步处理。然而，在模型的输出层，尤其是在文本生成任务中，通常需要将这些低维的嵌入向量转换回高维的、离散的表示形式，即1-hot编码，以便能够直接对应到词汇表中的具体单词。</p>
<p>然而，在实际应用中，直接从嵌入向量转换到1-hot编码并不常见，因为这样做会丢失嵌入向量所包含的丰富语义信息。相反，模型通常会利用嵌入向量来计算词汇表中每个单词的权重（或概率），并根据这些权重来选择输出单词。</p>
<h3>2. 利用权重引入随机性</h3>
<p>在文本生成过程中，为了增加生成文本的多样性和自然度，模型需要能够在多个可能的输出中选择一个。这通常是通过计算词汇表中每个单词的权重（或概率），并根据这些权重来随机选择单词来实现的。</p>
<p>在GPT-3等LLM中，温度参数（temperature parameter）被用来控制这种随机性的程度。温度参数是一个超参数，它可以调整权重分布的形状。当温度参数设置为0时，模型会倾向于总是选择权重最高的单词（即最可能的单词），这会导致生成的文本缺乏多样性。而当温度参数设置为1时，模型会根据权重的实际分布来随机选择单词，从而增加生成文本的多样性。</p>
<h3>3. 随机性来源的说明</h3>
<p>您提到的随机性来源仅存在于令牌解码过程中，而不存在于LLM本身是准确的。LLM本身通过其复杂的神经网络结构和大量的训练数据来学习语言的规律和模式。然而，在生成文本时，为了避免总是产生相同的输出，模型需要在解码过程中引入随机性。这种随机性是通过计算权重并根据这些权重来随机选择单词来实现的，而温度参数则提供了一种调整这种随机性程度的方法。</p>
<p>综上所述，标记解码过程中的随机性是通过计算词汇表中每个单词的权重，并根据这些权重来随机选择单词来实现的。在GPT-3等LLM中，温度参数被用来控制这种随机性的程度，从而增加生成文本的多样性和自然度。</p>
<p>GPT系列模型使用的最常见的标记化方案之一被称为字节对编码[75]。</p>
<h3>字节对编码（BPE）的工作原理</h3>
<ol>
<li>
<p><strong>初始词汇表</strong>：BPE从一个包含所有单个字符（如字母、标点符号等）的初始词汇表开始。</p>
</li>
<li>
<p><strong>频率统计</strong>：然后，它遍历训练数据，统计所有符号对的出现频率。</p>
</li>
<li>
<p><strong>合并最频繁符号对</strong>：选择出现频率最高的符号对，并将它们合并为一个新的“单词”（或标记）。这个新的“单词”被添加到词汇表中，而原始的符号对从统计中移除。</p>
</li>
<li>
<p><strong>迭代</strong>：这个过程重复进行，每次迭代都会合并一个新的符号对，直到达到预定的词汇表大小或迭代次数限制。</p>
</li>
<li>
<p><strong>应用</strong>：最终得到的词汇表用于将文本分割成标记（tokens），这些标记是词汇表中的单词或合并后的符号对。</p>
</li>
</ol>
<p>字节对编码（Byte Pair Encoding, BPE）是一种基于统计的压缩算法，最初用于数据压缩，后来被广泛应用于自然语言处理（NLP）中的词汇表构建和分词。以下是BPE工作原理的一个详细举例讲解：</p>
<h3>一、初始化</h3>
<ol>
<li><strong>准备数据</strong>：首先，需要准备一份训练语料，即一系列需要被处理的文本数据。</li>
<li><strong>拆分字符</strong>：将训练语料中的所有单词拆分成单个字符，并在每个单词的末尾添加一个特殊的结束标记（如<code>&lt;/w&gt;</code>），以标识单词的边界。</li>
<li><strong>构建初始词汇表</strong>：初始词汇表包含所有出现的字符以及结束标记<code>&lt;/w&gt;</code>。</li>
</ol>
<h3>二、迭代合并</h3>
<ol>
<li><strong>统计字符对频率</strong>：遍历训练语料，统计每对相邻字符（或子词）的出现频率。</li>
<li><strong>合并最高频字符对</strong>：选择出现频率最高的字符对进行合并，形成一个新的子词（或标记）。这个新的子词被添加到词汇表中，并更新词汇表中其他子词的出现频率。</li>
<li><strong>重复迭代</strong>：不断重复上述步骤，即统计频率、合并最高频字符对、更新词汇表，直到达到预定的词汇表大小、迭代次数限制，或者无法再找到更高频的字符对进行合并。</li>
</ol>
<h3>三、举例</h3>
<p>假设我们有以下训练语料及其单词频率：</p>
<ul>
<li>"low": 5次</li>
<li>"lowest": 2次</li>
<li>"new": 6次</li>
<li>"widest": 3次</li>
</ul>
<p><strong>初始步骤</strong>：</p>
<ol>
<li>
<p>拆分字符并添加结束标记：</p>
<ul>
<li>low -&gt; l o w </li>
<li>lowest -&gt; l o w e s t </li>
<li>new -&gt; n e w </li>
<li>widest -&gt; w i d e s t </li>
</ul>
</li>
<li>
<p>初始词汇表：['l', 'o', 'w', 'e', 's', 't', 'n', 'i', 'd', '']</p>
</li>
</ol>
<p><strong>第一轮迭代</strong>：</p>
<ol>
<li>
<p>统计字符对频率（部分示例）：</p>
<ul>
<li>('l', 'o') 出现7次</li>
<li>('o', 'w') 出现7次</li>
<li>('w', '') 出现11次（因为每个单词都以w结尾）</li>
<li>...</li>
</ul>
</li>
<li>
<p>合并最高频字符对('w', '')，得到新子词w，并更新词汇表：</p>
<ul>
<li>词汇表更新为：['l', 'o', 'w', 'e', 's', 't', 'n', 'i', 'd']</li>
</ul>
</li>
</ol>
<p><strong>后续迭代</strong>（简化版）：</p>
<p>继续迭代，合并其他高频字符对，如('l', 'o') -&gt; 'lo'，('n', 'e') -&gt; 'ne'，等等。每次合并后，都会更新词汇表和字符对频率统计。</p>
<p><strong>最终词汇表</strong>（假设经过多轮迭代后）：</p>
<ul>
<li>['est', 'new', 'low', 'wid', 'lo', 'w']</li>
</ul>
<h3>四、编码过程</h3>
<p>使用最终得到的词汇表对新的文本进行编码时，算法会从最长的子词开始，尝试匹配文本中的子字符串，并将其替换为对应的子词（标记）。如果文本中有未被匹配的子字符串，则可能需要将其拆分成更小的子字符串，或者用一个特殊的未知标记（如<code>&lt;unk&gt;</code>）来表示。</p>
<h3>五、总结</h3>
<p>BPE通过迭代合并高频字符对的方式，逐步构建出一个紧凑且高效的词汇表，从而能够在保持词汇丰富性的同时，减少词汇表的大小。这种方法在自然语言处理领域中被广泛应用，特别是在处理罕见词和未知词方面表现出色。</p>
<p>您所描述的字节对编码（Byte Pair Encoding, BPE）是GPT系列模型以及许多其他自然语言处理（NLP）任务中广泛使用的标记化方案之一。BPE 是一种数据压缩算法，但它在NLP领域中被改编用于构建有效的词汇表，特别是处理罕见词和未知词（OOV，Out-Of-Vocabulary）问题。</p>
<h3>BPE在GPT系列模型中的应用</h3>
<p>在GPT系列模型中，BPE帮助构建了一个高效的词汇表，该词汇表能够处理广泛的词汇，包括罕见词和未知词。通过将常见的字符组合合并成标记，BPE减少了词汇表的大小，同时保持了模型处理文本的能力。</p>
<h3>BPE在其他领域的应用</h3>
<p>正如您所提到的，BPE的概念可以扩展到其他类型的数据模式，如图像、视频和机器人动作。在这些场景中，数据可以被视为顺序数据，并通过类似的方式进行“标记化”：</p>
<ul>
<li>
<p><strong>图像</strong>：图像可以被分割成一系列图像补丁（patches），这些补丁可以类似于文本中的单词，用于训练图像生成或处理模型。</p>
</li>
<li>
<p><strong>视频</strong>：视频可以看作是一系列帧的集合，这些帧可以按照时间顺序进行“标记化”，以训练视频处理或生成模型。</p>
</li>
<li>
<p><strong>机器人动作</strong>：机器人的动作序列可以看作是一系列指令或动作单元，这些单元可以被“标记化”以训练机器人执行特定任务的模型。</p>
</li>
</ul>
<p>在这些情况下，BPE或类似的技术（如SentencePiece、WordPiece等）可以用于构建有效的词汇表或标记集，从而帮助模型更好地理解和处理这些数据模式。</p>
<p>生成模型:生成模型是一种学习从概率分布中抽样的模型，以创建与训练数据似乎来自相同分布的数据示例。例如，人脸生成模型可以生成无法与用于训练模型的真实图像集区分的人脸图像。这些模型可以被训练成有条件的，这意味着它们从条件分布中生成样本，条件分布以广泛的可能条件信息为条件。例如，性别条件面部生成器可以生成女性或男性面部的图像，其中所需的性别作为模型的条件输入。</p>
<p>判别模型:判别模型用于回归或分类任务。与生成模型相反，判别模型被训练来区分不同的类或类别。他们的重点在于学习输入空间中类之间的边界。</p>
<p>当生成模型学习从数据的分布中进行采样时，判别模型学习评估给定输入特征的输出标签的概率分布，或者(取决于模型的训练方式)学习评估输出概率分布的一些统计量，例如给定输入的预期输出。</p>
<p>Transformer架构是许多基础模型（Foundation Models）和大型语言模型（Large Language Models）的核心基础，它在这些模型的兴起中起到了至关重要的作用。以下讨论综合了来自多篇文献、在线博客、未公开报告以及Wikipedia的信息。</p>
<p>Transformer模型同时作用于一组嵌入的标记向量（token vectors），这些向量被称为上下文窗口（context window），表示为(x1, ..., xN)。Transformer架构的关键创新之处在于其采用了多头自注意力机制（Multi-Head Self-Attention Mechanism），这一机制最初在具有里程碑意义的论文[76]中被提出。</p>
<p>在多头自注意力机制中，每个注意力头（Attention Head）都会计算一个重要性权重向量，该向量表示了上下文窗口中的一个标记xi与同一窗口中其他标记xj之间的相关性强弱。每个注意力头通过不同的投影矩阵在计算重要性权重时，对数学上不同的相似性概念进行编码。这意味着每个头都可以从独特的角度理解标记之间的关系。</p>
<p>此外，Transformer架构允许所有标记和所有注意力头之间的并行训练（反向传播）和评估（前向传播）。这种并行性相比基于循环神经网络（RNNs）或长短期记忆网络（LSTMs）的模型，大大加快了训练和推理的速度。</p>
<p>总的来说，Transformer架构通过其高效的多头自注意力机制，实现了对文本数据的深入理解和快速处理，从而奠定了大型语言模型和许多基础模型成功的基础。</p>
<p>在数学上，Transformer模型中的一个注意力头（Attention Head）通过以下步骤将上下文窗口中的每个标记<code>xi</code>映射到一个“查询”<code>qi</code>，以及将上下文中的其他每个标记<code>xj</code>映射到一个“键”<code>kj</code>。这些映射是通过线性变换（即与权重矩阵相乘）完成的，具体地，$q_i = W^T_q x_i$和$k_j = W^T_k x_j$，其中$W_q$和$W_k$是查询和键的权重矩阵。</p>
<p>接下来，通过缩放点积（Scaled Dot Product）来衡量查询<code>qi</code>和键<code>kj</code>之间的相似性，即$q^T_i \frac{k_j}{\sqrt(d)}$，其中<code>d</code>是查询和键向量的维度。这个缩放因子<code>√d</code>有助于稳定梯度，使得训练过程更加平滑。</p>
<p>然后，对所有<code>j</code>应用softmax函数，得到权重$\alpha_{ij}$，这些权重表示<code>xi</code>在多大程度上“关注”<code>xj</code>。softmax函数的输出保证了所有<code>αij</code>的和为1，从而可以将它们解释为概率分布。</p>
<p>接下来，将上下文中的每个标记<code>xj</code>映射到一个“值”<code>vj</code>，即$v_j = W^T_v x_j$，其中<code>W^v</code>是值的权重矩阵。</p>
<p>最后，位置<code>i</code>的注意力输出是通过将所有值<code>vj</code>按照注意力权重<code>αij</code>加权求和得到的，即$\sum_{j} \alpha_{ij}v_j$。这个加权和构成了Transformer模型中注意力机制的核心输出，它反映了在给定查询<code>qi</code>时，上下文信息对当前标记<code>xi</code>的加权贡献。</p>
<p>Transformer模型中的注意力机制之所以成功，其中一个关键原因是它可以通过将上述步骤并行化为矩阵运算来高效地利用GPU和TPU等硬件进行计算。具体来说，注意力机制的计算可以表示为
$$
\text{attn}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$
其中<code>Q</code>、<code>K</code>和<code>V</code>分别是查询、键和值的矩阵表示，而<code>dk</code>是键向量的维度。这种矩阵形式的计算极大地加速了Transformer模型的训练和推理过程。</p>
<p>在Transformer模型中，<code>Q</code>（查询）、<code>K</code>（键）、<code>V</code>（值）是矩阵，它们的行分别由<code>q_i^T</code>、<code>k_i^T</code>、<code>v_i^T</code>组成，这些向量分别对应于上下文窗口中的不同标记（tokens）。模型中的每个注意力头（Attention Head）都独立地进行这种计算，但使用不同的<code>W^q</code>、<code>W^k</code>、<code>W^v</code>矩阵来编码不同类型的注意力。每个注意力头的输出随后被拼接起来，并通过一个带有跳跃连接（skip connection）的全连接ReLU层进行传递，然后再次通过跳跃连接进行归一化，以产生注意力层的输出。</p>
<p>Transformer模型的大小通常由以下几个因素决定：</p>
<ol>
<li>上下文窗口的大小（即同时处理的标记数量）。</li>
<li>每层的注意力头数量。</li>
<li>每个注意力头中注意力向量的大小。</li>
<li>堆叠的注意力层数量。</li>
</ol>
<p>以GPT-3为例，其上下文窗口大小为2048个标记（大约对应于1500个文本单词），每个注意力层有96个注意力头，每个注意力头中的注意力向量为128维，且模型中共有96个堆叠的注意力层。</p>
<p>基本的多头注意力机制本身并不对数据强加任何固有的序列感或方向性。然而，在自然语言应用中，Transformer模型经常被用作序列预测器，通过对输入标记序列施加位置编码（Positional Encoding）来实现。这些模型以自回归方式应用于标记序列，即它们预测序列中的下一个标记，将该标记添加到上下文窗口中，并重复此过程。</p>
<p>自回归Transformer模型的工作原理如下：</p>
<ul>
<li>首先，对输入序列中的每个标记施加位置编码，以便模型能够了解每个标记在序列中的位置。</li>
<li>然后，模型从序列的第一个标记（或特定的起始标记）开始，计算该标记的注意力表示。</li>
<li>接着，模型使用这个注意力表示来预测序列中的下一个标记。</li>
<li>预测出的标记被添加到模型的上下文窗口中，作为下一个预测步骤的输入之一。</li>
<li>这个过程一直重复，直到序列结束或达到某个预定的预测长度。</li>
</ul>
<p>通过这种方式，Transformer模型能够逐步构建对输入序列的深入理解，并生成相应的输出序列。</p>
<h1>Transformer模型中的注意力头机制</h1>
<p>在Transformer模型中，注意力头（Attention Head）通过一系列步骤将上下文窗口中的每个标记（token）映射并计算其与其他标记之间的相关性，从而生成一个加权后的输出。以下是详细的步骤：</p>
<h2>1. 线性变换</h2>
<p>对于上下文窗口中的每个标记$xi$，注意力头首先通过线性变换将其映射到三个不同的向量空间：查询（query）、键（key）和值（value）。这些映射是通过与权重矩阵相乘来完成的：</p>
<ul>
<li>查询向量 $qi$：$$ q_i = W_q^T x_i $$</li>
<li>键向量 $kj$（对于每个$j$）：$$ k_j = W_k^T x_j $$</li>
<li>值向量 $vj$（对于每个$j$）：$$ v_j = W_v^T x_j $$
其中，$W_q$、$W_k$、$W_v$ 是学习得到的权重矩阵，分别用于生成查询、键和值向量。</li>
</ul>
<h2>2. 缩放点积注意力</h2>
<p>接下来，注意力头通过缩放点积来衡量查询向量 $qi$ 和每个键向量 $kj$ 之间的相似性。缩放因子 $√d$（其中 $d$ 是键向量的维度）用于稳定梯度：
$$
\text{similarity}<em>{ij} = \frac{q_i^T k_j}{\sqrt{d}}
$$
然后，对所有 $j$ 应用softmax函数，得到权重 $\alpha</em>{ij}$，这些权重表示 $xi$ 在多大程度上“关注” $xj$：
$$
\alpha_{ij} = \text{softmax}\left(\frac{q_i^T k_j}{\sqrt{d}}\right)
$$
softmax函数的输出保证了所有 $\alpha_{ij}$ 的和为1，从而可以将它们解释为概率分布。</p>
<h2>3. 加权求和</h2>
<p>最后，注意力头将上下文中的每个值向量 $vj$ 按照对应的注意力权重 $\alpha_{ij}$ 进行加权求和，得到位置 $i$ 的注意力输出 $oi$：
$$
o_i = \sum_{j} \alpha_{ij} v_j
$$
这个加权和构成了Transformer模型中注意力机制的核心输出，它反映了在给定查询 $qi$ 时，上下文信息对当前标记 $xi$ 的加权贡献。</p>
<h2>4. 矩阵形式的计算</h2>
<p>Transformer模型中的注意力机制之所以高效，是因为它可以将上述步骤并行化为矩阵运算。具体来说，注意力机制的计算可以表示为：
$$
\text{attn}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$
其中，$Q$、$K$ 和 $V$ 分别是查询、键和值的矩阵表示，它们的行分别由 $q_i^T$、$k_i^T$、$v_i^T$ 组成。$d_k$ 是键向量的维度。这种矩阵形式的计算极大地加速了Transformer模型的训练和推理过程。</p>
<h2>5. 多头注意力</h2>
<p>在Transformer模型中，通常会使用多个注意力头（即多头注意力）来并行捕获不同类型的注意力信息。每个注意力头都独立地进行上述计算，但使用不同的权重矩阵 $W_q$、$W_k$、$W_v$。每个注意力头的输出随后被拼接起来，并通过一个带有跳跃连接（skip connection）的全连接ReLU层进行传递，然后再次通过跳跃连接进行归一化，以产生注意力层的最终输出。</p>
<p>中文解释如下：</p>
<p><strong>变压器（Transformer）模型的大小通常由以下因素量化</strong>：</p>
<ol>
<li>
<p><strong>上下文窗口大小</strong>：这指的是模型能够同时考虑的标记（或位置）数量。例如，GPT-3的上下文窗口大小为2048个标记，这大约相当于1500个文本单词，使得模型能够处理大量的文本并捕捉长距离依赖关系。</p>
</li>
<li>
<p><strong>每层注意力头的数量</strong>：每个注意力层可以包含多个注意力头。GPT-3的每一层都有96个注意力头，这允许模型同时关注输入序列的不同部分，并捕捉不同方面的上下文信息。</p>
</li>
<li>
<p><strong>每个头中注意力向量的大小</strong>：这指的是查询、键和值向量的维度。GPT-3的每个注意力头使用128维的向量，这在捕捉有意义的表示和保持计算效率之间取得了平衡。</p>
</li>
<li>
<p><strong>堆叠的注意力层数量</strong>：通过堆叠多个注意力层，模型能够构建更复杂的输入表示。GPT-3有96个堆叠的注意力层，这有助于其生成连贯且信息丰富的文本。</p>
</li>
</ol>
<p><strong>基本的多头注意力机制本身并不在数据中强加任何固有的序列或方向性感觉</strong>。然而，在自然语言应用中，变压器模型通常通过对输入标记序列施加位置编码来作为序列预测器使用。位置编码为每个标记提供了一个唯一的位置标识符，使模型能够理解序列的顺序。</p>
<p><strong>然后，这些模型以自回归的方式应用于标记序列</strong>，这意味着它们预测序列中的下一个标记，将该标记添加到其上下文窗口中，并重复此过程。自回归解码使模型能够生成遵循自然语言分布的连贯文本。</p>
<p><strong>自回归模型（Autoregressive Models）</strong>：自回归的概念在许多领域中被广泛应用，作为一种表示随机过程的方式，这些随机过程的输出在因果上依赖于之前的输出。自回归模型利用过去数据的一个窗口来预测序列中的下一个数据点。然后，这个窗口向前滑动一个位置，递归地将预测的数据点纳入窗口，并从窗口中移除最旧的数据点。模型再次预测序列中的下一个数据点，并无限重复这个过程。</p>
<p><strong>经典线性自回归模型</strong>：如自回归滑动平均模型（ARMA）和带有外生输入的自回归滑动平均模型（ARMAX）是标准的统计工具，其历史可以追溯到至少20世纪70年代。这些模型建模的概念首先被应用于深度学习模型中，特别是循环神经网络（RNNs），然后是长短期记忆网络（LSTMs），它们都是可学习的非线性自回归模型。</p>
<p><strong>Transformer模型与自回归框架</strong>：尽管Transformer模型本身并不固有地是自回归的，但它们经常被改造成自回归框架以用于文本预测任务。这是因为Transformer模型中的解码器部分天然适合进行序列预测，它逐步生成序列中的每个元素，同时考虑已经生成的元素作为上下文。</p>
<p><strong>GPT系列模型</strong>：以GPT家族为例，这些模型在原始Transformer模型的基础上进行了修改，移除了Transformer的编码器块，仅保留了解码器块。这种设计的优势在于，它减少了模型参数的数量（接近一半），同时减少了编码器和解码器中学习的冗余信息。在训练过程中，GPT模型试图从标记化语料库<code>X = (x1, ..., xn)</code>中生成输出标记，以最小化长度为N的上下文窗口内的负对数似然（Negative Log-Likelihood, NLL）：</p>
<p>$$
\text{LLLM} = -\sum_{i} \log P(x_i | x_{i-N}, ..., x_{i-1})
$$
这导致了一个大型预训练模型，该模型在给定上下文窗口中的标记时，能够自回归地预测下一个可能的标记。尽管GPT家族模型功能强大，但其单向自回归的特性意味着这些模型在处理如阅读理解等双向任务时可能表现不佳，因为这些任务需要同时考虑序列的上下文信息。</p>
<p><strong>双向任务与Transformer的双向编码器</strong>：为了克服这一限制，研究人员开发了如BERT这样的模型，它们使用Transformer的双向编码器来同时考虑序列中的前后文信息。这种设计使得模型在需要理解整个序列的上下文时更加有效，但也意呀着在生成任务（如文本生成）中，需要额外的解码器或自回归机制来逐步生成输出。</p>
<h3>掩码自编码（Masked Auto-Encoding）</h3>
<p><strong>背景</strong>：
GPT系列模型存在单向限制，即它们只能根据之前的文本进行预测，而无法利用后文信息。为了解决这个问题，并允许模型进行双向预测，BERT等模型采用了掩码自编码的方法。</p>
<p><strong>实现方式</strong>：</p>
<ol>
<li><strong>架构变化</strong>：添加了一个双向编码器，这使得模型能够同时考虑文本前后的信息。</li>
<li><strong>预训练目标</strong>：引入了一个新的预训练任务，即掩码语言建模（Masked Language Modeling, MLM）。在这个任务中，模型会随机掩盖语料库中的一部分词汇，并要求模型预测这些被掩盖的词汇。</li>
</ol>
<p><strong>效果</strong>：
通过这种方式，模型被鼓励去学习一个词汇周围的上下文，而不仅仅是序列中的下一个可能词汇。这有助于模型更好地理解语言的复杂性和多样性。</p>
<h3>对比学习（Contrastive Learning）</h3>
<p><strong>背景</strong>：
视觉-语言基础模型，如CLIP，通常使用与大型语言模型不同的训练方法。这些方法不鼓励模型进行明确的预测行为，而是采用对比表示学习。</p>
<p><strong>实现方式</strong>：</p>
<ol>
<li><strong>目标</strong>：学习一个联合嵌入空间，在这个空间中，来自不同输入模态（如文本和图像）的相似样本对应该比不相似的样本对更接近。</li>
<li><strong>训练目标</strong>：许多视觉-语言模型（Visual-Language Models, VLMs）的训练目标都是某种形式的对比损失函数（contrastive loss function）。这个函数鼓励模型将相似的样本对拉得更近，而将不相似的样本对推得更远。</li>
</ol>
<p><strong>效果</strong>：
对比学习使得模型能够更好地理解不同模态之间的关联，并在没有直接标注的情况下学习有用的表示。这对于构建能够处理多种类型输入（如文本和图像）的模型特别有用。</p>
<p>综上所述，掩码自编码和对比学习是两种不同的学习方法，它们分别用于解决语言模型和视觉-语言模型中的不同挑战。掩码自编码通过掩盖部分输入并要求模型进行预测来学习上下文表示，而对比学习则通过比较不同模态的样本对来学习联合嵌入空间。</p>
<p>公式(3)：从向量 <code>v</code> 到向量 <code>u</code> 的损失函数</p>
<p>$$
\ell^{(v \rightarrow u)}<em>i = - \log \frac{\exp(\text{sim}(v_i, u_i) / \tau)}{\sum</em>{k=1}^{PN} \exp(\text{sim}(v_i, u_k) / \tau)}
$$</p>
<p>其中，<code>sim</code> 表示相似度函数，<code>τ</code> 是温度参数，<code>PN</code> 是负样本的数量。</p>
<p>公式(4)：从向量 <code>u</code> 到向量 <code>v</code> 的损失函数</p>
<p>$$
\ell^{(u \rightarrow v)}<em>i = - \log \frac{\exp(\text{sim}(u_i, v_i) / \tau)}{\sum</em>{k=1}^{PN} \exp(\text{sim}(u_i, v_k) / \tau)}
$$</p>
<p>公式(5)：两个方向损失函数的加权和</p>
<p>$$
L = \frac{1}{N} \sum_{i=1}^{N} \left[ \lambda \ell^{(v \rightarrow u)}_i + (1 - \lambda) \ell^{(u \rightarrow v)}_i \right]
$$</p>
<p>其中，<code>λ</code> 是一个超参数，用于平衡两个方向的损失，<code>N</code> 是样本的总数。</p>
<ol>
<li><strong>目标函数的作用</strong>：</li>
</ol>
<ul>
<li>用于训练图像和文本编码器，以保留真实图像和文本对之间的互信息。</li>
<li>使模型学习到图像和文本之间的关联，使相似的图像和文本在嵌入空间中彼此靠近。</li>
</ul>
<ol start="2">
<li><strong>符号解释</strong>：</li>
</ol>
<ul>
<li><code>ui</code> 和 <code>vi</code>：分别表示第 <code>i</code> 个编码后的文本和图像，<code>i</code> 取值范围为 <code>1, ..., N</code>。</li>
<li><code>sim</code>：表示文本和图像嵌入之间的余弦相似度。</li>
<li><code>τ</code>：是一个温度参数，用于调整softmax函数的平滑程度。</li>
</ul>
<ol start="3">
<li><strong>CLIP模型中的应用</strong>：</li>
</ol>
<ul>
<li>使用了一个对称的交叉熵损失函数。</li>
<li>最终损失是两个损失组件的平均值，且每个组件的权重相等（<code>λ = 0.5</code>）。</li>
</ul>
<ol start="4">
<li><strong>总结</strong>：</li>
</ol>
<ul>
<li>是多模态学习中的一个重要组件，允许模型学习到不同模态之间的共同表示。</li>
<li>通过优化这个目标函数，模型可以在各种多模态任务中表现出色。</li>
</ul>
<p>以下是对扩散模型（Diffusion Models）的详尽阐释，包括对其中的公式进行解读：</p>
<p><strong>概述</strong>：</p>
<p>扩散模型，特别是应用于图像生成的模型（例如DALL-E2），构成了本次调查中考虑的另一类基础模型。这类模型与大型语言模型及多模态模型（例如VLMs）有所不同。尽管扩散模型的概念在先前的研究中已经确立，但[91]中提出的扩散概率模型使得该方法得到了广泛的推广。</p>
<p><strong>扩散概率模型详解</strong>：</p>
<p>扩散概率模型是一种深度生成模型，它依赖于迭代的前向和反向过程进行训练</p>
<p>详细解释扩散模型中的前向过程和反向过程，以及相关的损失函数，可以归纳如下：</p>
<h3>一、前向过程（Forward Process）</h3>
<p><strong>定义与目的</strong>：
前向过程是扩散模型的第一步，其目标是在输入数据x0上逐步添加高斯噪声，直至数据完全转化为噪声。这一过程遵循马尔可夫链的特性，即每个时间步t的状态xt仅依赖于前一个状态xt-1。</p>
<p><strong>数学形式</strong>：
前向过程的数学表达可以描述为：
$$
q(x_{1:T} | x_0) := \prod_{t=1}^T q(x_t | x_{t-1})
$$
其中，$q(x_t | x_{t-1})$表示在给定$x_{t-1}$的条件下$x_t$的分布。根据扩散模型的设定，这一分布通常被假设为正态分布，具体形式为：
$$
q(x_t | x_{t-1}) = N(x_t; \sqrt{1 - \beta_t}x_{t-1}, \beta_tI)
$$
其中，$\beta_t$是一个预定的或固定的超参数，控制着每一步添加的噪声量。随着t的增加，$\beta_t$通常也会增加，意味着添加的噪声量逐渐增大。</p>
<p><strong>过程解释</strong>：</p>
<ul>
<li>从$x_0$开始，每一步都根据上一步的结果$x_{t-1}$和当前的噪声水平$\beta_t$生成新的状态$x_t$。</li>
<li>这一过程持续进行，直到达到某个预定的步数T，此时$x_T$将接近于纯高斯噪声。</li>
</ul>
<h3>二、反向过程（Reverse Process）</h3>
<p><strong>定义与目的</strong>：
反向过程是扩散模型的第二步，也是生成数据的关键步骤。其目标是从噪声数据$x_T$出发，逐步去除噪声，最终恢复出原始数据$x_0$。这一过程同样遵循马尔可夫链的特性。</p>
<p><strong>数学形式</strong>：
反向过程的数学表达可以描述为：
$$
p_\theta(x_{0:T}) := p(x_T) \prod_{t=1}^T p_\theta(x_{t-1} | x_t)
$$
其中，$p_\theta(x_{t-1} | x_t)$表示在给定$x_t$的条件下，通过模型参数$\theta$预测的$x_{t-1}$的分布。在扩散模型中，这一分布通常也被假设为正态分布，具体形式为：
$$
p_\theta(x_{t-1} | x_t) = N(x_{t-1}; \mu_\theta(x_t, t), \Sigma_\theta(x_t, t))
$$
其中，$\mu_\theta(x_t, t)$和$\Sigma_\theta(x_t, t)$分别表示预测的均值和方差，它们通过模型学习得到。</p>
<p><strong>过程解释</strong>：</p>
<ul>
<li>从噪声数据$x_T$开始，模型逐步去除噪声，生成一系列中间状态$x_{T-1}, x_{T-2}, \ldots, x_1$。</li>
<li>最终，模型生成的数据$x_0$应该尽可能接近原始输入数据。</li>
</ul>
<h3>三、损失函数（Loss Function）</h3>
<p><strong>定义与目的</strong>：
损失函数用于衡量模型预测结果与真实结果之间的差异，并指导模型的训练过程。在扩散模型中，损失函数通常基于变分推断中的证据下界（ELBO）进行简化。</p>
<p><strong>具体形式</strong>：
扩散模型的损失函数可以表示为多个KL散度的和，具体形式可能因不同的模型设计而有所差异。但一个常见的简化形式是：
$$
L = E_q\left[D_{KL}(q(x_T | x_0) || p(x_T))\right] + \sum_{t&gt;1} D_{KL}(q(x_{t-1} | x_t, x_0) || p_\theta(x_{t-1} | x_t)) - \log p_\theta(x_0 | x_1)
$$
其中，$D_{KL}(q||p)$表示KL散度，用于衡量分布q和分布p之间的差异。</p>
<p><strong>解释</strong>：</p>
<ul>
<li>第一项衡量了最终噪声分布$q(x_T | x_0)$与先验噪声分布$p(x_T)$之间的差异。</li>
<li>第二项和第三项共同衡量了反向过程中每一步预测分布与真实分布之间的差异。通过最小化这些差异，模型能够学习到如何有效地去除噪声并恢复原始数据。</li>
</ul>
<p>综上所述，扩散模型通过前向过程逐步添加噪声将输入数据转化为噪声数据，然后通过反向过程逐步去除噪声恢复原始数据。在训练过程中，模型通过最小化损失函数来学习如何有效地执行这两个过程。</p>
<p><strong>总结</strong>：</p>
<p>扩散模型，尤其是图像生成领域的模型，是一类强大的基础模型。它们通过迭代的前向和反向过程进行训练，其中前向过程逐步添加噪声，而反向过程则学习去除噪声并生成图像。训练过程采用了一种简化的损失函数，该函数与变分生成模型中的证据下界损失函数密切相关。</p>
<h2>B.大型语言模型(LLM)示例和历史</h2>
<p>LLM有数十亿个参数，并在数万亿个令牌上进行训练。这种大规模使得GPT-2[92]和BERT[1]等模型分别在Winograd模式挑战[93]和通用语言理解评估(GLUE)[94]基准测试中实现了最先进的性能。它们的后继者包括GPT-3[2]、LLaMA[95]和PaLM[96]，它们在参数数量(通常现在超过1000亿个)、上下文窗口的大小(通常现在超过1000个令牌)和训练数据集的大小(通常现在有10s tb的文本)方面都有很大的增长。GPT-3是在Common Crawl数据集上训练的。</p>
<p>Common Crawl包含超过12年的网络抓取的公开可用数据，包括原始网页数据、元数据和文本摘录。法学硕士也可以是多语言的。例如，ChatGLM-6B和GLM-130B[97]是一个拥有1300亿个参数的双语(英语和汉语)预训练语言模型。LLM还可以进行微调，这是一个使用领域特定数据调整模型参数以使LLM的性能与特定用例保持一致的过程。例如，GPT-3和GPT-4[3]已经使用带有人类反馈(RLHF)的强化学习进行了微调。</p>
<h2><strong>Vision Transformers（ViT）概述</strong>：</h2>
<p>Vision Transformer（ViT）[98]–[100]是一种专为计算机视觉任务设计的Transformer架构，这些任务包括图像分类、分割和目标检测。ViT将图像视为一系列图像块（或称为“tokens”）。</p>
<p><strong>图像Token化过程</strong>：
在图像Token化过程中，图像首先被分割成固定大小的块。然后，这些块被展平成一维向量，这被称为线性嵌入。为了捕获图像块之间的空间关系，每个token都会添加位置信息，这一过程称为位置嵌入。</p>
<p><strong>Transformer编码器与自注意力机制</strong>：
融入了位置编码的图像tokens会被输入到Transformer编码器中。自注意力机制使模型能够捕获输入数据中的长期依赖和全局模式。</p>
<p><strong>大型ViT模型</strong>：
本文重点关注那些具有大量参数的ViT模型。例如，ViT-G[101]扩展了ViT模型，拥有20亿（2B）参数。另外，ViT-e[102]拥有40亿（4B）参数。而ViT-22B[103]是一个拥有220亿参数的视觉Transformer模型，它被用于PaLM-E和PaLI-X[104]中，并有助于机器人任务。</p>
<p><strong>DINO：自监督学习ViT</strong>：
DINO[105]是一种用于训练ViT的自监督学习方法。它是一种无标签的知识蒸馏形式。知识蒸馏是一种学习框架，其中较小的模型（学生网络）被训练以模仿较大、更复杂的模型（教师网络）的行为。两个网络具有相同的架构但参数集不同。给定一个固定的教师网络，学生网络通过学习其参数来最小化相对于学生网络参数的交叉熵损失。</p>
<p><strong>神经网络架构</strong>：
该神经网络架构由ViT或ResNet[106]作为主干，以及一个包括多层感知（MLP）层的投影头组成。使用DINO学习的自监督ViT特征包含有关图像语义分割的明确信息，包括场景布局和对象边界，其清晰度是使用监督学习的ViT或卷积网络（convnets）无法达到的。</p>
<p><strong>DINOv2：预训练的视觉模型</strong>：
DINOv2[107]提供了各种预训练的视觉模型，这些模型是使用不同的视觉Transformer（ViT）在[107]中引入的LVD-142M数据集上训练的。它使用判别式自监督方法在配备有8个V100-32GB GPU的20个节点计算集群上进行训练。DINOv2提供了图像级（如检测）或像素级（如分割）的各种视觉特征。</p>
<p><strong>SAM：零样本可提示图像分割</strong>：
SAM[59]提供了零样本可提示图像分割功能。这将在第四节中详细讨论。</p>
<h2><strong>多模态视觉语言模型（VLMs）概述</strong>：</h2>
<p>“多模态”指的是模型能够接受不同“模态”的输入，例如图像、文本或音频信号。视觉语言模型（VLM）是一种多模态模型，它同时接收图像和文本作为输入。在机器人应用中，常用的VLM是对比语言-图像预训练（CLIP）[4]。</p>
<p><strong>CLIP模型</strong>：
CLIP提供了一种比较文本描述和图像之间相似性的方法。它使用大规模的互联网图像-文本对数据来捕获图像和文本之间的语义信息。CLIP模型架构包含一个文本编码器[92]和一个图像编码器（一个修改过的视觉Transformer，即ViT），它们被联合训练以最大化图像和文本嵌入之间的余弦相似性。CLIP结合对比学习、语言模型和视觉特征编码器，实现了零样本图像分类。</p>
<p><strong>BLIP模型</strong>：
BLIP[108]专注于多模态学习，通过在预训练期间联合优化三个目标来实现这一点。这些目标包括图像-文本对比损失、图像-文本匹配损失和语言建模损失。该方法通过引导式标题利用嘈杂的网络数据，从而增强了训练过程。</p>
<p><strong>CLIP2模型</strong>：
CLIP2[109]旨在构建良好对齐的、基于实例的文本-图像-点代理。它使用跨模态对比目标来学习语义和实例级对齐的点云表示。</p>
<p><strong>FILIP模型</strong>：
FILIP[110]专注于在多模态学习中实现更精细级别的对齐。它融入了一种跨模态晚期交互机制，该机制利用视觉和文本token之间的token级最大相似性。这种机制指导对比目标并改善了视觉和文本信息之间的对齐。</p>
<p><strong>FLIP模型</strong>：
FLIP[111]为CLIP提出了一种简单且更高效的训练方法。在训练过程中，FLIP随机遮罩并移除大量图像块。这种方法旨在提高CLIP的训练效率，同时保持其性能。</p>
<h2>E. Embodied Multimodal Language Models</h2>
<p><strong>具身化多模态语言模型概述</strong>：
具身化代理是一种与虚拟或物理世界进行交互的AI系统，例如虚拟助手或机器人。具身化语言模型是基础模型，它们将现实世界中的传感器和执行器模态融入预训练的大型语言模型中。典型的视觉语言模型是针对一般的视觉语言任务进行训练的，如图像描述或视觉问题回答。</p>
<p><strong>PaLME模型</strong>：
PaLME[6]是一种多模态语言模型，它不仅在互联网规模的通用视觉语言数据上进行了训练，还在具身化、机器人数据上进行了同时训练。为了将模型与真实世界的传感器模态连接起来，PaLME的架构将（连续的）输入（如图像、低级状态或3D神经场景表示）注入到仅解码器的语言模型的语言嵌入空间中，使模型能够同时对文本和其他模态进行推理。主要的PaLM-E版本是基于PaLM LLM[96]和ViT[103]构建的。ViT将图像转换为嵌入向量序列，这些向量通过仿射变换投影到语言嵌入空间中。整个模型是从预训练的LLM和ViT模型开始端到端训练的。作者还探索了不同的策略，如冻结LLM而只训练ViT，但这会导致性能下降。在给定多模态输入时，PaLM-E的输出是文本，通过自回归方式进行解码。为了将这个输出连接到机器人进行控制，可以使用语言条件的短期策略。在这种情况下，PaLM-E充当高级控制策略。实验表明，单个PaLM-E不仅是一个通用的视觉语言模型，还能够在多个机器人具身化上执行许多不同的机器人任务。该模型表现出正迁移，即同时在互联网规模的语言、通用视觉语言和具身化领域进行训练相比在单个任务上进行训练可以获得更高的性能。</p>
<h2><strong>F. 视觉生成模型</strong>：</h2>
<p>如OpenAI的DALLE[112]和DALL-E2[88]等网络规模的扩散模型提供了零样本文本到图像的生成。它们在互联网上数百万个图像-描述对上进行训练。这些模型学习了一个语言条件的图像分布，可以使用给定的提示从该分布中生成图像。DALL-E2架构包括一个先验，它从文本描述中生成CLIP图像嵌入，以及一个解码器，它根据图像嵌入生成图像。</p>
<h1>机器人技术</h1>
<h2>机器人决策控制与策略学习</h2>
<h3>语言条件模仿学习概述</h3>
<p><strong>定义</strong>：
语言条件模仿学习是一种机器人学习策略，其中机器人通过学习一个目标条件策略πθ(at|st, l)，能够根据当前状态st（如机器人的关节角度、位置等）和语言指令l（如“拿起红色的杯子”）来输出动作at（如移动手臂）。这种方法结合了模仿学习和自然语言处理，使机器人能够理解并执行基于语言的任务。</p>
<p><strong>损失函数</strong>：
在语言条件模仿学习中，损失函数（Loss Function）用于量化模型预测的动作与真实演示动作之间的差异。常用的损失函数是最大似然估计，其目标是最大化在给定状态和语言指令下，模型输出正确动作的概率。具体形式如公式所示：
$$
\mathcal{L}<em>{GCIL}=\mathbb{E}</em>{(\tau,l)~D}\sum_{t=0}^{|\tau|}log\pi_{\theta}(a_{\theta}|s_t,l)
$$</p>
<p>该公式 $\mathcal{L}<em>{GCIL}=\mathbb{E}</em>{(\tau,l)\sim D}\sum_{t=0}^{|\tau|}\log\pi_{\theta}(a_t|s_t,l)$ 是语言条件模仿学习（Language-Conditioned Imitation Learning, LCIL）中损失函数的数学表达，它用于量化模型预测的动作与真实演示动作之间的差异。下面我将详细讲解这个公式的数学基础、原理和内容。</p>
<h3>数学基础</h3>
<ol>
<li>
<p><strong>期望（Expectation）</strong>：</p>
<ul>
<li>$\mathbb{E}_{(\tau,l)\sim D}$ 表示对随机变量 $(\tau,l)$ 在数据集 $D$ 上的分布求期望。在这里，$(\tau,l)$ 代表从数据集 $D$ 中随机抽取的一个演示轨迹和对应的语言指令。</li>
</ul>
</li>
<li>
<p><strong>求和（Summation）</strong>：</p>
<ul>
<li>$\sum_{t=0}^{|\tau|}$ 表示对轨迹 $\tau$ 中的每个时间步 $t$ 从 0 到 $|\tau|$（轨迹长度）进行求和。这反映了损失函数需要考虑轨迹中的每一个时间步。</li>
</ul>
</li>
<li>
<p><strong>对数概率（Log Probability）</strong>：</p>
<ul>
<li>$\log\pi_{\theta}(a_t|s_t,l)$ 表示在给定状态 $s_t$ 和语言指令 $l$ 的条件下，模型 $\pi_{\theta}$ 输出动作 $a_t$ 的对数概率。对数概率在优化过程中通常比原始概率更稳定，且能够将乘法转换为加法，便于计算。</li>
</ul>
</li>
</ol>
<h3>原理</h3>
<p>该公式的核心原理是<strong>最大似然估计（Maximum Likelihood Estimation, MLE）</strong>。在模仿学习中，我们的目标是找到一个策略模型 $\pi_{\theta}$，该模型能够在给定状态 $s_t$ 和语言指令 $l$ 的条件下，输出与演示数据中相同的动作 $a_t$。为了实现这一目标，我们最大化模型输出正确动作的概率，即最大化似然函数。</p>
<p>由于直接最大化似然函数可能涉及复杂的乘法运算，我们通常转而最大化其对数形式，即对数似然函数。这样做的好处是，对数似然函数将乘法运算转换为加法运算，同时保留了似然函数的单调性，因此最大化对数似然函数与最大化似然函数是等价的。</p>
<h3>内容</h3>
<ul>
<li>
<p><strong>数据集 $D$</strong>：包含一系列带有语言注释的演示轨迹 $(\tau,l)$。每个演示轨迹 $\tau$ 是一系列状态-动作对 $(s_t, a_t)$ 的序列，其中每个状态 $s_t$ 和动作 $a_t$ 都与相应的语言指令 $l$ 相关联。</p>
</li>
<li>
<p><strong>模型 $\pi_{\theta}$</strong>：待学习的策略模型，它接受当前状态 $s_t$ 和语言指令 $l$ 作为输入，并输出一个动作 $a_t$。模型的参数由 $\theta$ 表示，这些参数在训练过程中通过优化算法进行调整。</p>
</li>
<li>
<p><strong>损失函数 $\mathcal{L}_{GCIL}$</strong>：用于量化模型预测与真实演示之间的差异。通过最小化这个损失函数，我们可以训练出一个能够准确模仿演示数据的策略模型。损失函数的具体形式是对数似然函数的负值（在实际应用中，我们通常最小化负对数似然函数），即 $-\mathbb{E}<em>{(\tau,l)\sim D}\sum</em>{t=0}^{|\tau|}\log\pi_{\theta}(a_t|s_t,l)$。然而，在公式中直接给出了正数形式，这是为了强调我们是在最大化对数似然函数。</p>
</li>
</ul>
<p>综上所述，该公式通过最大似然估计的原理，利用对数概率和期望操作，定义了一个用于语言条件模仿学习的损失函数。通过最小化这个损失函数（或等价地，最大化其对数似然函数），我们可以训练出一个能够根据当前状态和语言指令来输出正确动作的策略模型。</p>
<h3>在机器人操控任务中的应用</h3>
<p><strong>应用场景</strong>：
语言条件模仿学习在机器人操控任务中具有广泛的应用前景。例如，在家庭服务机器人、工业生产线上的自动化机器人、医疗辅助机器人等领域，机器人需要理解和执行人类给出的复杂语言指令，如“将物体从A处移动到B处”、“打开/关闭开关”等。</p>
<p><strong>工作流程</strong>：</p>
<ol>
<li><strong>数据收集</strong>：首先，需要收集大量的带有语言注释的演示数据。这些数据通常包括机器人执行特定任务时的状态信息、动作信息以及对应的语言指令。</li>
<li><strong>模型训练</strong>：使用收集到的演示数据来训练目标条件策略模型πθ。在训练过程中，模型会学习如何将语言指令映射到相应的动作上，以便在给定的状态下执行正确的任务。</li>
<li><strong>测试与部署</strong>：训练完成后，将模型部署到实际机器人上进行测试。在测试阶段，机器人会接收到一系列语言指令，并根据这些指令执行相应的动作。如果模型表现良好，则可以将其用于实际的生产或服务场景中。</li>
</ol>
<p><strong>主要挑战</strong>：</p>
<ol>
<li><strong>数据获取</strong>：获取足够数量的高质量演示数据是一个挑战。这通常需要大量的时间和人力成本。</li>
<li><strong>分布偏移</strong>：在闭环策略下，机器人的反馈可能导致其进入训练数据中未充分覆盖的状态空间区域，从而产生分布偏移问题。这可能会影响机器人的性能和稳定性。</li>
<li><strong>语言理解</strong>：自然语言具有复杂性和多样性，机器人需要准确理解人类给出的语言指令，并将其映射到正确的动作上。这要求机器人具备强大的自然语言处理能力。（配对演示和语言教学比较昂贵，可以从未标记的游戏数据中学习）</li>
</ol>
<h3>总结</h3>
<p>语言条件模仿学习为机器人提供了一种理解和执行基于语言指令的复杂任务的能力。通过结合模仿学习和自然语言处理，机器人可以在各种操控任务中表现出更高的灵活性和适应性。然而，这种方法也面临着数据获取、分布偏移和语言理解等挑战。未来的研究将致力于解决这些问题，以进一步提升机器人在语言条件模仿学习领域的能力。</p>
<h3>多上下文模仿学习</h3>
<p><strong>背景与动机</strong>：
在文献[28]中，作者提出了多上下文模仿学习（MCIL），这是一种针对非结构化数据使用语言条件模仿学习的方法。该方法的提出是为了解决在复杂、多变的环境中使用单一上下文信息进行模仿学习的局限性。</p>
<p><strong>框架基础</strong>：
MCIL框架建立在重标记模仿学习（relabeled imitation learning）和带标签的指令跟随（labeled instruction following）的基础上。重标记模仿学习通过重新标记演示数据来改进策略，而带标签的指令跟随则允许模型根据语言指令来执行任务。</p>
<p><strong>数据集要求</strong>：
MCIL假设能够访问多个上下文模仿数据集。这些数据集可能包括目标图像演示（goal image demonstrations）、语言目标演示（language goal demonstrations）或独热任务演示（one-hot task demonstrations）。这些多样化的数据集为模型提供了丰富的上下文信息，有助于提升其在不同情境下的泛化能力。</p>
<p><strong>训练过程</strong>：</p>
<ul>
<li><strong>编码上下文</strong>：MCIL使用一个共享的潜在空间来编码来自不同数据集的上下文信息。对于每种上下文，都使用相应的编码器将其编码到潜在空间中。</li>
<li><strong>训练策略</strong>：在共享潜在空间的基础上，MCIL同时训练一个单一的目标条件策略（latent goal-conditioned policy）。这意味着模型能够根据不同的上下文（即不同的任务或目标）来动态调整其行为。</li>
<li><strong>计算损失</strong>：通过计算所有数据集上的平均值来得到目标条件模仿损失（goal-conditioned imitation loss）。这种平均损失的方式有助于模型在多个上下文之间找到平衡点，从而避免过拟合于某个特定数据集。</li>
<li><strong>端到端训练</strong>：策略和目标编码器是端到端训练的，这意味着它们在整个训练过程中是同时优化的，以确保它们能够协同工作以达到最佳性能。</li>
</ul>
<h3>应对数据标注挑战</h3>
<p>在语言条件模仿学习中，数据标注是一个重要的挑战。为了应对这一挑战，研究者们提出了利用基础模型（foundation models）来提供反馈的方法。</p>
<p><strong>基础模型反馈</strong>：
在文献[115]中，作者提出使用预训练的基础模型来标注演示数据，从而提供反馈。这种方法的好处在于它不需要人工标注大量数据，而是利用已经训练好的模型来自动完成这一任务。</p>
<p><strong>部署与微调</strong>：</p>
<ul>
<li><strong>部署新任务</strong>：为了将训练好的策略部署到新的任务或环境中，可以使用随机生成的指令来运行策略。此时，预训练的基础模型会对演示进行标注并提供反馈。</li>
<li><strong>策略微调</strong>：这些配对的指令-演示数据还可以用于策略的微调。通过在新数据上继续训练策略，可以进一步提升其在新任务或环境中的表现。</li>
</ul>
<p>总之，MCIL及其相关方法为解决语言条件模仿学习中的挑战提供了新的思路。通过利用多上下文数据集和基础模型反馈机制，这些方法有望提高模型在复杂、多变环境中的泛化能力和鲁棒性。</p>
<p>当然，我们可以继续深入解读CLIPort和PerAct（Perceiver-Actor）这两种方法，它们都是针对基于视觉的语言条件模仿学习框架，特别是在机器人操作任务中的应用。</p>
<h3>CLIPort</h3>
<p><strong>背景与动机</strong>：
CLIPort [25] 提出了一种基于视觉的语言条件模仿学习方法，旨在解决特定语言指令下的操作任务。它的核心在于将CLIP（Contrastive Language-Image Pre-training）的语义理解能力与Transporter [116]（一种用于物体运输的视觉操作模型）的空间精确度相结合。</p>
<p><strong>两流架构</strong>：
CLIPort采用了一种两流架构，其中一流负责处理图像的视觉信息，利用CLIP模型进行语义理解；另一流则负责处理空间信息，利用Transporter模型进行精确的空间定位和操作。这种结合使得CLIPort能够执行语言指定的操作任务，而无需显式表示物体的姿态或实例分割。</p>
<blockquote>
<p>CLIP（一个能理解图片和文字之间关系的模型）和Transporter（一个擅长在视觉上移动物体的模型）。</p>
</blockquote>
<p><strong>特点与限制</strong>：</p>
<ul>
<li><strong>语义与空间的结合</strong>：CLIPort成功地将语义概念与精确的空间推理相结合，提高了模型在理解复杂语言指令并进行相应操作时的能力。</li>
<li><strong>2D观测与行动空间</strong>：然而，CLIPort受限于2D观测和行动空间，这意味着它只能处理二维图像和二维空间内的操作。</li>
</ul>
<h3>PerAct（Perceiver-Actor）</h3>
<p><strong>背景与动机</strong>：
为了克服CLIPort的局限性，PerAct [27] 被提出，它使用3D体素（voxels）来表示观测和行动空间，并利用体素块（voxel patches）的3D结构来进行高效的语言条件行为克隆。PerAct旨在通过少量的演示来模仿6自由度（6-DoF）的操作任务。</p>
<p><strong>3D观测与行动空间</strong>：</p>
<ul>
<li><strong>多视图观测</strong>：与2D方法（如CLIPort）相比，PerAct支持多视图观测，这有助于更全面地理解环境。</li>
<li><strong>6-DoF行动空间</strong>：PerAct的行动空间是6自由度的，这意味着机器人可以在三维空间中进行更复杂的操作。</li>
</ul>
<p><strong>模型架构</strong>：</p>
<ul>
<li><strong>Perceiver Transformer</strong>：PerAct使用Perceiver Transformer来处理输入的语言目标和RGB-D体素观测，并输出离散化的行动。它只使用CLIP的语言编码器来编码语言目标。</li>
<li><strong>行动预测</strong>：通过检测下一个最佳的体素行动，PerAct能够预测出相应的6-DoF姿态、夹爪开启状态和避障行动。</li>
</ul>
<p><strong>训练过程</strong>：</p>
<ul>
<li><strong>监督学习</strong>：PerAct通过监督学习进行训练，其训练数据包括体素观测、语言目标和关键帧行动序列的配对。在训练过程中，随机采样一个元组，并给定观测和目标，让模型预测关键帧行动。</li>
</ul>
<h3>总结</h3>
<p>CLIPort和PerAct都利用了CLIP的语义理解能力，但它们在处理空间信息和行动空间方面有所不同。CLIPort侧重于2D空间内的操作，而PerAct则扩展到3D空间，支持多视图观测和6-DoF行动空间。这两种方法都展示了将语义表示融入空间环境对于有效机器人交互的重要性。CLIPort和PerAct分别通过Transporter和Perceiver Transformer来实现空间推理，从而完成了从语言指令到实际操作的映射。</p>
<p>Voltron 是一个针对机器人领域的语言驱动表示学习框架，它旨在从视频和对应的语言描述（即字幕）中捕获语义、空间和时序表示。这个框架通过结合不同的学习方法来克服单一方法（如对比学习或掩码自编码）的局限性，从而提供更全面和有效的表示。</p>
<h3>核心思想</h3>
<ol>
<li>
<p><strong>语义、空间和时序表示的融合</strong>：</p>
<ul>
<li><strong>对比学习</strong>（Contrastive Learning）：擅长捕获语义表示，但可能丢失空间关系。</li>
<li><strong>掩码自编码</strong>（Masked Autoencoding, MAE）：能够捕获空间和结构信息，但可能不擅长直接捕获语义内容。</li>
<li><strong>Voltron</strong>：通过折衷语言条件下的视觉重建（以获取局部空间表示）和基于视觉的语言生成（以捕获语义表示），Voltron成功地将这两种方法结合起来，从而同时获得语义和空间表示。</li>
</ul>
</li>
<li>
<p><strong>多模态编码器与解码器</strong>：</p>
<ul>
<li>Voltron采用一个多模态编码器，该编码器接收视频及其对应的语言描述作为输入。</li>
<li>编码器的输出随后被解码以重构被掩码上下文中的一个或多个帧。这种重构过程不仅帮助模型学习空间结构，还通过语言条件增强了语义理解能力。</li>
</ul>
</li>
<li>
<p><strong>动态组件与语言前缀</strong>：</p>
<ul>
<li>在MAE的基础上，Voltron添加了一个动态组件，该组件通过语言前缀来条件化MAE编码器。这允许模型根据语言指令来动态地调整其视觉表示。</li>
<li>通过考虑多个帧，Voltron还能够捕获时序信息，这对于理解连续动作和预测未来状态至关重要。</li>
</ul>
</li>
</ol>
<h3>应用任务</h3>
<p>Voltron框架支持多种机器人任务，包括：</p>
<ul>
<li><strong>抓握能力预测</strong>：预测物体是否适合被抓取。</li>
<li><strong>单任务视动控制</strong>：根据视觉输入执行单一动作。</li>
<li><strong>指代表达定位</strong>：将语言描述与视频中的具体对象或区域关联起来。</li>
<li><strong>语言条件模仿</strong>：根据语言指令模仿演示行为。</li>
<li><strong>意图评分</strong>：评估语言指令与机器人实际行为之间的一致性。</li>
</ul>
<h3>挑战与解决方案</h3>
<p>尽管语言条件模仿学习在机器人领域具有巨大潜力，但将其部署到真实机器人上仍面临诸多挑战：</p>
<ul>
<li><strong>泛化问题</strong>：由于模型是在演示数据集上进行监督学习的，它们可能无法很好地泛化到未见过的场景或任务中。</li>
<li><strong>分布偏移</strong>：训练数据与实际应用中的数据分布可能存在差异，导致模型性能下降。</li>
</ul>
<p>为了提高模型的鲁棒性和适应性，可以采用以下技术：</p>
<ul>
<li><strong>数据增强</strong>：通过修改训练数据来增加模型的泛化能力。</li>
<li><strong>域适应</strong>：使模型能够适应不同领域或环境的数据分布。</li>
</ul>
<p>综上所述，Voltron通过结合语义、空间和时序表示学习，为机器人提供了一个强大的语言驱动表示学习框架。尽管在实际应用中仍面临挑战，但通过不断的研究和技术改进，语言条件模仿学习有望在机器人领域发挥更大的作用。</p>
<p>CACTI（可能是一个缩写，但具体含义未在文中直接给出，可能是指某个特定的研究框架或系统）是一个创新性的框架，旨在通过使用如Stable Diffusion等基础模型来增强机器人学习中的可扩展性。Stable Diffusion是一种强大的视觉生成模型，能够在各种场景中生成高质量的图像。CACTI通过引入四个关键阶段——数据收集、数据增强、视觉表征学习和模仿策略训练，来实现这一目标。下面是对这四个阶段的详细解读：</p>
<h3>1. 数据收集阶段</h3>
<p>在这一阶段，CACTI主要收集有限的在域（in-domain）专家演示数据。这些数据通常是针对特定任务或场景的专业操作记录，例如厨房环境中的烹饪或清洁任务。由于这些数据是专家提供的，因此它们通常具有较高的质量和准确性，但数量可能有限。</p>
<h3>2. 数据增强阶段</h3>
<p>为了克服数据量的限制并增加数据的多样性，CACTI采用了数据增强技术。特别是，它利用视觉生成模型（如Stable Diffusion）来增强数据，通过引入场景和布局的变化来生成更多的训练样本。这种方法不仅增加了训练数据的数量，还提高了数据的多样性，有助于模型更好地泛化到不同的场景和任务中。</p>
<h3>3. 视觉表征学习阶段</h3>
<p>在这一阶段，CACTI利用在域外（out-of-domain）数据上预训练的零样本视觉表征模型来提高训练效率。这些预训练模型已经学会了如何从图像中提取有用的特征表示，这些特征表示对于后续的任务学习非常有用。通过将这些预训练模型应用到增强后的数据集上，CACTI能够更有效地学习视觉表征，为后续的模仿策略训练提供坚实的基础。</p>
<h3>4. 模仿策略训练阶段</h3>
<p>最后，在模仿策略训练阶段，CACTI使用增强后的数据集和压缩的视觉表征作为输入来训练一个通用的多任务策略。模仿学习是一种通过模仿专家行为来学习策略的方法，它允许模型从大量演示数据中学习如何执行各种任务。由于CACTI采用了数据增强和视觉表征学习的技术，因此它能够学习到更加鲁棒和泛化的策略，这些策略可以在不同的场景和任务中有效地执行。</p>
<h3>总结</h3>
<p>CACTI是一个针对厨房环境设计的多任务和多场景机器人操作框架，它通过数据收集、数据增强、视觉表征学习和模仿策略训练四个阶段来提高机器人的学习效率和泛化能力。通过使用Stable Diffusion等视觉生成模型进行数据增强，并利用预训练的视觉表征模型来提高训练效率，CACTI能够在仿真和真实环境中实现高效且鲁棒的机器人操作。这些技术的结合使得CACTI能够从一个广泛的环境中学习，并展现出强大的泛化能力。</p>
<h3>论文解读：任务规范和语言辅助强化学习的最新进展</h3>
<hr>
<h4>一、任务规范的多样化探索</h4>
<ol>
<li>
<p><strong>MimicPlay</strong>:</p>
<ul>
<li><strong>基本思想</strong>：MimicPlay 提出了一种新的模仿学习算法，叫做分层模仿学习算法。</li>
<li><strong>数据来源</strong>：
<ul>
<li><strong>高层次计划</strong>：从人类游戏数据中学习，这些数据帮助算法理解任务的大致步骤。</li>
<li><strong>低层次运动指令</strong>：从少量遥控操作示范中学习，这些示范帮助算法执行具体动作。</li>
</ul>
</li>
<li><strong>优势</strong>：结合这两种数据源的优势，这种算法能够显著降低训练视觉运动策略的成本。</li>
<li><strong>应用</strong>：训练后，算法能基于一次人类视频示范执行新任务。</li>
</ul>
</li>
<li>
<p><strong>MUTEX</strong>:</p>
<ul>
<li><strong>探索内容</strong>：MUTEX 研究如何在多模态任务规范（视频、图像、文本和音频）中学习统一的策略。</li>
<li><strong>性能提升</strong>：通过跨模态学习，比单一模态的基准方法显示出更好的性能。</li>
</ul>
</li>
</ol>
<hr>
<h4>二、语言辅助的强化学习</h4>
<ol>
<li>
<p><strong>强化学习 (RL) 概述</strong>：</p>
<ul>
<li><strong>定义</strong>：RL 是一种通过与环境交互来优化策略的方法，目的是最大化奖励函数。</li>
<li><strong>过程</strong>：通常在模拟环境中进行，有时会使用物理机器人硬件的数据来实现从模拟到现实的转移。</li>
<li><strong>特点</strong>：
<ul>
<li>与最优控制有密切关系。</li>
<li>不需要人类示范，理论上可以达到超越人类的性能。</li>
</ul>
</li>
<li><strong>反馈机制</strong>：通过从环境中收到的奖励信号，指导机器人学习哪些动作能带来有利结果。</li>
</ul>
</li>
<li>
<p><strong>Adaptive Agent (AdA)</strong>：</p>
<ul>
<li><strong>基础模型</strong>：一种预训练在多种任务上的RL基础模型。</li>
<li><strong>目标</strong>：快速适应开放式的3D问题（如导航、协调和劳动分工任务）。</li>
<li><strong>方法</strong>：
<ul>
<li>使用变压器架构（Transformer-XL），结合大规模注意力机制和长期依赖关系捕捉。</li>
<li>在XLand环境中收集多样的数据（1040个可能任务）。</li>
<li>使用蒸馏技术扩展模型参数至超过500M。</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Palo等人的研究</strong>：</p>
<ul>
<li><strong>目标</strong>：通过集成大语言模型（LLMs）和视觉语言模型（VLMs）来增强RL。</li>
<li><strong>任务类型</strong>：机器人操作任务。</li>
<li><strong>方法</strong>：
<ul>
<li>将复杂任务分解为简单子任务（由LLM完成）。</li>
<li>基于变压器的代理根据当前环境状态执行最佳子任务。</li>
<li>结合监督学习和强化学习进行训练。</li>
</ul>
</li>
</ul>
</li>
</ol>
<hr>
<h3>关键点总结</h3>
<ol>
<li><strong>多模态任务规范的学习</strong>：利用不同数据源（视频、图像、文本、音频）的互补优势，通过跨模态学习提升策略性能。</li>
<li><strong>分层模仿学习</strong>：结合高层次计划和低层次运动指令，降低训练成本并提高任务执行能力。</li>
<li><strong>语言辅助的RL</strong>：将LLM和VLM集成到RL框架中，解决探索、经验重用、技能调度和观察学习等核心挑战。</li>
<li><strong>快速适应性</strong>：通过预训练和大规模注意力机制，RL代理能在未见过的环境中进行快速适应和优化。</li>
</ol>
<p>这些研究展示了通过多样化的数据源和先进的模型架构，如何在复杂任务中提升智能体的学习和适应能力。这些进展对于推动通用人工智能的发展至关重要。</p>
<h3>语言-图像目标条件值学习的详细讲解</h3>
<hr>
<h4>1. R3M (Reusable Representation for Robotic Manipulations)</h4>
<p><strong>概念</strong>：预训练的视觉表示用于机器人操作任务。</p>
<p><strong>例子</strong>：一个机器人手臂需要学会执行不同的操控任务，比如抓取物品或移动物体。</p>
<ul>
<li><strong>预训练数据</strong>：使用Ego4D等多样化的人类视频数据进行预训练，学习视觉表示。</li>
<li><strong>时间对比学习</strong>：通过时间对比学习捕捉时间依赖性，使得相邻时间帧的图像距离最小化。</li>
<li><strong>视频-语言对齐</strong>：捕捉场景的语义特征，比如物体及其关系。</li>
<li><strong>应用</strong>：预训练的视觉表示可以用于不同的下游操控任务，无需重新训练视觉模块。</li>
</ul>
<h3>详细讲解 Value-Implicit Pretraining (VIP)</h3>
<hr>
<h4>概述</h4>
<p>Value-Implicit Pretraining (VIP) 是一种自监督学习方法，旨在从视频中学习视觉目标条件的值函数和表示。VIP 使用时间对比学习来捕捉视频中的时间依赖性，不需要视频与语言的对齐。该方法主要应用于机器人操控任务，通过预训练的视觉表示模型来推导奖励模型，支持零样本条件下的奖励指定。</p>
<hr>
<h4>实现细节</h4>
<h5>1. 数据预处理与预训练</h5>
<ul>
<li><strong>预训练数据</strong>：使用未标注的人类视频进行预训练。这些视频没有包含具体的动作信息，但提供了丰富的视觉内容。</li>
<li><strong>时间对比学习</strong>：VIP 通过时间对比学习方法，训练视觉表示，以捕捉视频帧之间的时间依赖性。</li>
</ul>
<h5>2. 时间对比学习目标</h5>
<ul>
<li><strong>时间对比目标</strong>：在训练过程中，VIP 使用一个时间对比目标（time-contrastive objective），确保相同轨迹中的起始帧和目标帧表示更为接近，而中间帧表示被推开。</li>
<li><strong>递归单步时间差异最小化</strong>：VIP 通过递归单步时间差异最小化的方法，使得起始帧和目标帧之间的表示更加接近，捕捉长时间跨度任务帧之间的依赖关系，同时保持相邻帧之间的局部时间平滑性。</li>
</ul>
<h5>3. 视觉目标条件值函数学习</h5>
<ul>
<li><strong>隐式值函数定义</strong>：VIP 中的值函数是通过距离嵌入（distance embedding）隐式定义的，不直接依赖于具体的动作信息。</li>
<li><strong>奖励模型推导</strong>：通过预训练的视觉表示模型，VIP 可以推导出视觉目标条件的奖励模型，用于下游任务中的奖励指定。</li>
</ul>
<h5>4. 零样本奖励指定</h5>
<ul>
<li><strong>零样本学习</strong>：VIP 支持在零样本条件下，通过预训练的视觉表示指定奖励。即使在没有具体标注数据的情况下，VIP 也能够根据视频内容推断出合适的奖励信号。</li>
</ul>
<hr>
<h3>关键实现步骤</h3>
<ol>
<li>
<p><strong>数据收集与预处理</strong>：</p>
<ul>
<li>收集未标注的人类视频，确保视频内容具有多样性，能够覆盖不同的场景和任务。</li>
<li>对视频进行帧分割，准备用于时间对比学习的输入数据。</li>
</ul>
</li>
<li>
<p><strong>时间对比学习目标</strong>：</p>
<ul>
<li>设定时间对比学习目标，通过递归单步时间差异最小化的方法，确保相同轨迹中的起始帧和目标帧表示更为接近，而中间帧表示被推开。</li>
</ul>
</li>
<li>
<p><strong>视觉目标条件值函数学习</strong>：</p>
<ul>
<li>通过自监督学习方法，训练视觉表示模型，捕捉视频帧之间的时间依赖性和语义信息。</li>
<li>使用距离嵌入隐式定义值函数，不依赖具体动作信息。</li>
</ul>
</li>
<li>
<p><strong>奖励模型推导</strong>：</p>
<ul>
<li>从预训练的视觉表示中推导出视觉目标条件的奖励模型，用于下游任务中的奖励指定。</li>
</ul>
</li>
<li>
<p><strong>零样本奖励指定</strong>：</p>
<ul>
<li>在零样本条件下，通过预训练的视觉表示模型，推断出合适的奖励信号，支持下游任务的策略学习和执行。</li>
</ul>
</li>
</ol>
<hr>
<h3>应用实例</h3>
<p><strong>实例 1</strong>：家庭助理机器人</p>
<ul>
<li><strong>任务</strong>：根据视觉目标条件完成家庭清洁任务。</li>
<li><strong>过程</strong>：
<ul>
<li>机器人通过摄像头获取房间的视觉信息，使用预训练的视觉表示模型分析场景。</li>
<li>通过时间对比学习目标，捕捉视频帧之间的时间依赖性，生成视觉目标条件值函数。</li>
<li>在没有具体动作信息的情况下，推导出合适的奖励信号，指导机器人执行清洁任务。</li>
</ul>
</li>
</ul>
<p><strong>实例 2</strong>：仓库管理机器人</p>
<ul>
<li><strong>任务</strong>：根据视觉目标条件完成货物搬运任务。</li>
<li><strong>过程</strong>：
<ul>
<li>机器人通过摄像头获取货架和货物的视觉信息，使用预训练的视觉表示模型分析场景。</li>
<li>通过时间对比学习目标，捕捉视频帧之间的时间依赖性，生成视觉目标条件值函数。</li>
<li>在没有具体标注数据的情况下，推导出合适的奖励信号，指导机器人执行搬运任务。</li>
</ul>
</li>
</ul>
<hr>
<h3>关键点总结</h3>
<ol>
<li><strong>自监督学习与时间对比学习</strong>：VIP 通过自监督学习方法，使用时间对比学习目标，捕捉视频帧之间的时间依赖性，不需要具体的动作信息和标注数据。</li>
<li><strong>隐式值函数定义</strong>：VIP 的值函数通过距离嵌入隐式定义，不直接依赖于具体的动作信息，增强了模型的泛化能力。</li>
<li><strong>零样本奖励指定</strong>：VIP 支持在零样本条件下，通过预训练的视觉表示模型推导出奖励信号，减少对标注数据的依赖。</li>
<li><strong>实际应用</strong>：VIP 在家庭助理机器人和仓库管理机器人等任务中，通过预训练的视觉表示和时间对比学习目标，推导出合适的奖励信号，指导机器人执行具体任务。</li>
</ol>
<blockquote>
<p>假设我们有一段视频，展示了一个机器人从起点到达终点的过程。视频的每一帧都是一个时间点的快照。VIP 的目标是通过学习这个视频，捕捉机器人在不同时间点上的状态变化，而不是具体的动作细节。</p>
</blockquote>
<blockquote>
<ul>
<li><strong>距离嵌入</strong>：VIP 学习到的表示可以将相似时间点的帧放在一起，而将不相似的时间点的帧分开。例如，视频的起点和终点帧会被VIP的模型表示得非常接近，因为它们在同一个轨迹上。中间的帧则会被表示得较远，因为它们不直接连接起点和终点。</li>
<li><strong>值函数的定义</strong>：通过这种方式，VIP 的值函数能够隐式地表示从起点到终点的价值，而不需要明确知道机器人是如何移动的。值函数的定义基于视觉表示的距离关系，而不是具体的动作序列。</li>
</ul>
</blockquote>
<h3>语言-图像值学习 (LIV) 详细讲解</h3>
<hr>
<h4>概述</h4>
<p>Language-Image Value Learning (LIV) 是一种以控制为中心的视觉-语言表示方法，它通过使用语言对齐视频来学习多模态的视觉-语言值函数和表示。LIV 泛化了之前的 VIP 方法，通过结合视觉和语言信息，训练出一个通用的值函数，用于机器人操控任务。</p>
<hr>
<h4>实现细节</h4>
<h5>1. 数据预处理与预训练</h5>
<ul>
<li><strong>预训练数据</strong>：使用大型人类视频数据集（如 EPIC-KITCHENS）进行预训练。这些数据集包含了大量的日常活动视频，并附有文本注释，描述了视频中的活动和对象。</li>
<li><strong>文本注释对齐</strong>：视频中的每一帧都附有描述其内容的文本注释，确保视觉信息和语言信息的对齐。</li>
<li><strong>预训练目标</strong>：通过对比学习目标（类似于 CLIP 中使用的互信息图像-文本对比学习目标），训练视觉和语言的联合表示。该目标旨在最大化匹配的图像-文本对之间的相似性，最小化不匹配对之间的相似性。</li>
</ul>
<h5>2. 预训练模型结构</h5>
<ul>
<li><strong>视觉表示网络</strong>：使用卷积神经网络（CNN）提取视频帧中的视觉特征。</li>
<li><strong>语言表示网络</strong>：使用变压器（Transformer）或其他自然语言处理模型提取文本注释的语言特征。</li>
<li><strong>对比学习模块</strong>：通过对比学习模块，将视觉特征和语言特征映射到同一特征空间中，以最大化匹配的图像-文本对之间的相似性。</li>
</ul>
<h5>3. 值函数学习</h5>
<ul>
<li><strong>目标条件值函数</strong>：LIV 通过自监督学习的方式，训练一个目标条件值函数，该值函数通过对比学习目标，学习在给定目标条件下的视觉表示。</li>
<li><strong>时间对比学习</strong>：LIV 通过时间对比学习方法，捕捉长时间跨度任务帧之间的依赖关系，确保模型能够处理长时间任务。</li>
</ul>
<h5>4. 策略学习</h5>
<ul>
<li><strong>预训练表示冻结</strong>：在策略学习过程中，预训练的视觉和语言表示保持冻结，不参与训练。</li>
<li><strong>简单多层感知器（MLP）</strong>：在预训练表示的基础上，使用一个简单的多层感知器（MLP）进行策略网络的学习。MLP 接受预训练的视觉和语言特征作为输入，输出机器人在当前状态下应执行的动作。</li>
<li><strong>策略学习目标</strong>：通过强化学习或监督学习，优化策略网络，使其能够在给定的任务条件下，执行最优的动作序列。</li>
</ul>
<h5>5. 微调与应用</h5>
<ul>
<li><strong>微调数据</strong>：使用小规模的领域内机器人数据集进行微调，以便在具体的应用场景中，细化语言和视觉表示的上下文相关性。</li>
<li><strong>任务执行</strong>：在实际任务中，机器人根据文本描述（语言目标）或图像（图像目标）获取任务指令，使用预训练的视觉和语言表示，通过策略网络输出具体的动作序列，完成任务。</li>
</ul>
<hr>
<h3>关键实现步骤</h3>
<ol>
<li>
<p><strong>数据收集与预处理</strong>：</p>
<ul>
<li>收集并预处理包含文本注释的多样化视频数据集。</li>
<li>对视频进行帧分割，并将每一帧与对应的文本注释对齐。</li>
</ul>
</li>
<li>
<p><strong>视觉-语言表示预训练</strong>：</p>
<ul>
<li>使用卷积神经网络和变压器模型分别提取视觉和语言特征。</li>
<li>通过对比学习目标，将视觉特征和语言特征映射到同一特征空间中，训练联合表示。</li>
</ul>
</li>
<li>
<p><strong>目标条件值函数学习</strong>：</p>
<ul>
<li>使用自监督学习方法，训练目标条件值函数，捕捉任务帧之间的时间依赖性。</li>
<li>通过时间对比学习，确保模型能够处理长时间跨度的任务。</li>
</ul>
</li>
<li>
<p><strong>策略网络训练</strong>：</p>
<ul>
<li>在预训练表示的基础上，使用多层感知器进行策略网络的学习。</li>
<li>冻结预训练的视觉和语言表示，优化策略网络，使其能够在给定任务条件下执行最优动作。</li>
</ul>
</li>
<li>
<p><strong>微调与应用</strong>：</p>
<ul>
<li>使用领域内机器人数据集进行微调，增强语言和视觉表示的上下文相关性。</li>
<li>在实际任务中，通过预训练的表示和策略网络执行具体的机器人操作任务。</li>
</ul>
</li>
</ol>
<hr>
<h3>应用实例</h3>
<p><strong>实例 1</strong>：家庭助理机器人</p>
<ul>
<li><strong>任务</strong>：根据语言指令“请把桌子上的红色杯子放到厨房的柜子里”。</li>
<li><strong>过程</strong>：
<ul>
<li>机器人获取任务指令，并通过预训练的语言表示网络理解指令内容。</li>
<li>通过视觉表示网络识别桌子上的红色杯子。</li>
<li>使用策略网络规划从桌子到厨房柜子的路径，并执行抓取和放置任务。</li>
</ul>
</li>
</ul>
<p><strong>实例 2</strong>：仓库管理机器人</p>
<ul>
<li><strong>任务</strong>：根据图像目标，将特定物品从货架上移到打包区。</li>
<li><strong>过程</strong>：
<ul>
<li>机器人接收包含目标物品图像的任务指令。</li>
<li>通过视觉表示网络识别货架上的目标物品。</li>
<li>使用策略网络规划抓取和搬运路径，并执行任务。</li>
</ul>
</li>
</ul>
<p>通过这些具体实现细节和实例，可以更好地理解 LIV 方法是如何在机器人操作任务中应用的。</p>
<hr>
<h4>其他相关方法和应用</h4>
<ol>
<li><strong>SayCan</strong>：结合大语言模型和实际世界中的学习，通过语言模型提供任务基础，并使用学习到的可行性函数实现世界基础。</li>
</ol>
<ul>
<li><strong>例子</strong>：机器人需要根据高层指令完成多个子任务，并根据可行性函数确定哪些操作是可执行的。</li>
</ul>
<ol start="2">
<li><strong>Inner Monologue</strong>：研究与环境反馈闭环的大语言模型在机器人规划中的应用。</li>
</ol>
<ul>
<li><strong>例子</strong>：机器人在执行任务时，通过环境反馈（如成功检测）调整其计划。</li>
</ul>
<ol start="3">
<li><strong>Text2Motion</strong>：基于语言的长时间跨度机器人操作规划框架。</li>
</ol>
<ul>
<li>
<p><strong>例子</strong>：机器人需要根据语言指令找到一系列动作，确保这些动作在几何上是可行的。</p>
<ul>
<li>
<p>在每个时间步计算与每个技能相关的分数($S_{LMM}$)。任务规划问题是通过最大化给定语言指令和初始状态的技能序列的可能性来找到一个技能序列。在Text2Motion中，作者提出验证生成的长视界计划在符号上是正确的，在几何上是可行的。因此，几何可行性分数($S_{geo}$)被定义为序列中所有技能获得奖励的概率。为了计算总分，将LLM分数乘以几何可行性分数($S_{Skill} = S_{LMM}·S_{geo}$)。</p>
</li>
</ul>
<h3>VoxPoser [34]</h3>
<p>VoxPoser 是一个用于机器人操作任务的框架，它通过构建3D价值地图来将可操作性和约束嵌入到感知空间中。以下是它的工作方式：</p>
<ol>
<li>
<p><strong>输入</strong>：VoxPoser 接收来自环境的RGB-D观测数据和语言指令。</p>
</li>
<li>
<p><strong>生成代码</strong>：利用大型语言模型，VoxPoser 生成代码，这些代码与视觉-语言模型进行交互，从而提取一系列3D可操作性地图和约束地图。</p>
</li>
<li>
<p><strong>组合地图</strong>：生成的可操作性地图和约束地图被组合在一起，形成3D价值地图。</p>
</li>
<li>
<p><strong>应用</strong>：这些价值地图作为目标函数，用于引导运动规划器合成日常操作任务的轨迹。这意味着机器人可以根据这些地图中提供的信息，无需任何预先训练或指令，就能够有效地执行复杂的操作任务。</p>
</li>
</ol>
<h3>Reward Shaping Using CLIP</h3>
<p>在这篇论文中，提出了使用 CLIP 进行奖励塑形的方法，重点考虑了机器人操作任务。下面是它的工作方式：</p>
<ol>
<li>
<p><strong>目标描述和像素输入</strong>：模型接收由目标文本描述和原始像素观测组成的场景描述。</p>
</li>
<li>
<p><strong>使用 CLIP</strong>：利用 CLIP 模型来将场景中的对象与目标文本描述中的空间关系规则联系起来。</p>
</li>
<li>
<p><strong>奖励塑形</strong>：通过这种方式，模型可以基于原始像素信息来塑造奖励信号，这些信号直接用于学习任务策略。</p>
</li>
<li>
<p><strong>应用</strong>：这种方法利用了 CLIP 等大规模视觉-语言模型的发展成果，设计了一个从目标文本描述和原始像素观测中生成任务奖励信号的框架，从而提高了任务策略学习的效率和精度。</p>
</li>
</ol>
<h3>Hierarchical Universal Language Conditioned Policies 2.0 (HULC++)</h3>
<p>这篇工作介绍了 HULC++，它也关注机器人操作任务。以下是它的工作方式：</p>
<ol>
<li>
<p><strong>自监督视觉-语言可操作性模型</strong>：HULC++ 使用自监督方法从现实世界中未结构化的离线数据学习通用的语言条件机器人技能。</p>
</li>
<li>
<p><strong>模型架构</strong>：可操作性模型采用编码器-解码器结构，其中有两个解码器头。这两个头共享相同的编码器，并且都依赖于输入的语言指令。</p>
</li>
<li>
<p><strong>预测任务</strong>：一个头预测图像上的点的分布，其中每个像素的可能性表示其可操作点。另一个头预测高斯分布，从中采样预测的深度。</p>
</li>
<li>
<p><strong>输出</strong>：给定视觉观测和语言指令，模型输出像素级热图，表示可操作区域和相应的深度图。</p>
</li>
</ol>
<p>这些方法展示了如何通过结合视觉和语言信息，从而使机器人能够更智能地执行复杂的操作任务，无论是通过构建价值地图来指导轨迹生成，还是利用大规模视觉-语言模型来塑造任务奖励信号和学习通用机器人技能。</p>
</li>
</ul>
<hr>
<h3>关键点总结</h3>
<ol>
<li><strong>预训练和自监督学习</strong>：通过使用大型人类视频数据集和时间对比学习方法，预训练视觉表示和值函数，减少对标注数据的依赖。</li>
<li><strong>多模态学习</strong>：结合语言和视觉信息，学习更丰富的表示和值函数，提高机器人对复杂任务的理解和执行能力。</li>
<li><strong>时间依赖性</strong>：通过时间对比学习捕捉长时间跨度的任务帧之间的依赖关系，使得模型能够处理长时间任务。</li>
<li><strong>实际应用</strong>：在策略学习中冻结预训练的视觉表示，结合简单的策略网络，实现高效的任务执行。</li>
</ol>
<p>这些方法展示了如何利用先进的学习技术和多模态数据，提升机器人在复杂任务中的学习和执行能力。</p>
<h3>Robot Task Planning using Large Language Models</h3>
<p>大型语言模型（LLMs）可以用于执行复杂的长时间跨度机器人任务的高级任务规划。</p>
<h4>1. 任务规范的语言指令</h4>
<p>在机器人系统中，LLMs可以用于高级任务规划的语言指令。例如，SayCan [32] 使用LLM进行语言中的高级任务规划，通过学习的值函数将这些指令与环境进行关联。</p>
<ul>
<li>
<p><strong>SayCan 的工作原理</strong>：</p>
<ul>
<li><strong>LLM应用于高级任务规划</strong>：SayCan利用LLM来理解和生成高级任务规划的语言指令。这些指令描述了机器人需要执行的任务，例如“将物体A放置在位置B”。</li>
</ul>
</li>
<li>
<p><strong>时间逻辑在机器人系统中的应用</strong>：</p>
<ul>
<li>[39] 中提出了自然语言（NL）到时间逻辑（TL）的转换。研究使用包含28,000个NL-TL对的数据集，并使用T5模型进行微调。这种方法有助于在机器人任务中强制执行时间上的要求。</li>
</ul>
</li>
<li>
<p><strong>任务子目标的LLM规划</strong>：</p>
<ul>
<li>在机器人导航任务中，LLMs经常用于计划任务的子目标。这种方法有助于机器人理解和执行复杂的导航路径。</li>
</ul>
</li>
<li>
<p><strong>直接任务描述到中间任务表示的翻译</strong>：</p>
<ul>
<li>[40] 中使用少量样本从自然语言任务描述直接翻译到中间任务表示。这种表示被任务与运动规划（TAMP）算法使用，共同优化任务和运动计划。自回归重新提示用于纠正合成和语义错误。</li>
</ul>
</li>
</ul>
<h4>2. 使用语言模型生成任务规划的代码</h4>
<p>传统的任务规划需要广泛的领域知识，并且搜索空间很大。LLMs可以用于生成实现高级任务所需的任务序列。</p>
<ul>
<li>
<p><strong>ProgPrompt 的工作原理</strong>：</p>
<ul>
<li>在ProgPrompt [41] 中，作者介绍了一种提示方法，使用LLMs直接生成动作序列，而无需额外的领域知识。</li>
<li><strong>LLM的提示包括</strong>：可用动作的规范、环境中的对象以及可执行的示例程序。</li>
</ul>
</li>
<li>
<p><strong>VirtualHome模拟器的应用</strong>：</p>
<ul>
<li>VirtualHome [129] 用作演示的模拟器，用于展示机器人在虚拟环境中执行任务的能力。</li>
</ul>
</li>
</ul>
<p>这些方法显示了如何利用LLMs强大的语言理解和生成能力，帮助机器人在复杂环境中理解任务指令，并有效地规划和执行任务。</p>
<h3>Code-as-Policies</h3>
<p><strong>Code-as-Policies [42]</strong> 探索了使用编写代码的LLMs来生成基于自然语言命令的机器人策略代码。该工作考虑了机器人操作和导航任务，使用了来自Everyday Robots的现实世界移动操作机器人。研究表明，LLMs可以重新定位为编写策略代码，通过表达处理感知输出并调用控制原语API的函数或反馈循环。</p>
<h4>主要思想和实现细节：</h4>
<ol>
<li>
<p><strong>任务背景</strong>：</p>
<ul>
<li>研究使用现实世界中的移动操作机器人，展示了LLMs如何生成策略代码，以执行机器人操作和导航任务。</li>
</ul>
</li>
<li>
<p><strong>方法与技术</strong>：</p>
<ul>
<li><strong>LLMs编写策略代码</strong>：通过使用LLMs生成程序代码，表达函数或反馈循环来处理感知输出和调用控制API。</li>
<li><strong>少样本提示</strong>：使用少量示例语言命令作为注释提供，并附带相应的策略代码。这些示例使模型能够在给定新命令时自动组合API调用并生成新的策略代码，而无需在此数据上进行额外训练。</li>
<li><strong>逻辑结构和第三方库的使用</strong>：利用经典的逻辑结构和第三方库（如NumPy和Shapely）执行算术操作，通过链接这些结构和使用上下文信息（行为常识），LLMs可以生成展示空间几何推理、适用于新指令的机器人策略，并为模糊描述（例如“更快”）提供精确的值（例如速度）。</li>
</ul>
</li>
<li>
<p><strong>策略的形式化生成</strong>：</p>
<ul>
<li>“代码作为策略”的概念形式化了使用语言模型生成的程序（LMPs）来生成机器人策略。这些策略可以表示反应式策略（如阻抗控制器）以及基于路径点的策略，例如基于视觉的拾取和放置或基于轨迹的控制。</li>
</ul>
</li>
<li>
<p><strong>效果和应用</strong>：</p>
<ul>
<li>该方法在多个实际机器人平台上的有效性得到了验证，表明LLMs可以生成复杂的代码结构以满足所需的策略要求。</li>
<li><strong>层次化代码生成过程</strong>：这一关键步骤涉及递归定义未定义的函数，使LLMs能够生成更复杂的代码结构。</li>
</ul>
</li>
</ol>
<p>通过这种方式，LLMs不仅可以理解和执行复杂的机器人任务，还能够通过生成的代码实现高效的机器人策略，从而扩展了机器人在现实世界中的应用能力。</p>
<p>在[43]中，作者提供了在机器人中使用ChatGPT的设计原则，并演示了llm如何帮助机器人功能快速推广到不同的形状因素。</p>
<p>这项工作考虑了机器人操作和空中导航任务。首先，定义一个高级机器人函数库，它映射到机器人可执行的多个原子任务。然后，制作一个提示符，其中包括这些函数，以及任务描述中所需的约束。然后，ChatGPT提供特定于给定机器人配置和任务的可执行代码。然后，用户可以对生成的代码进行评估，并向llm提供适当的反馈和修改后的提示，进一步帮助改进和生成安全且可部署在物理机器人上的程序。研究表明，这种方法可以应用于模拟和现实世界中的多种形状因素。</p>
<h2>D.上下文学习对决策的影响</h2>
<h3>In-context Learning (ICL)</h3>
<ol>
<li>
<p><strong>定义与特点</strong></p>
<ul>
<li><strong>无参数优化</strong>：ICL不需要参数优化，而是依赖于在提示中包含的一组示例（提示的概念）。</li>
<li><strong>提示工程</strong>：与提示工程密切相关，在自然语言处理（NLP）中广泛使用。</li>
</ul>
</li>
<li>
<p><strong>技术与方法</strong></p>
<ul>
<li><strong>Chain-of-Thought方法</strong>
<ul>
<li><strong>定义</strong>：ICL中的一种突出技术，通过执行一系列中间步骤来解决复杂的多步问题。</li>
<li><strong>优势</strong>：使模型能够生成类似人类认知过程的逐步解释。</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>ICL的挑战</strong></p>
<ul>
<li><strong>歧义与解释问题</strong>：涉及如何理解和解释提示。</li>
<li><strong>领域特定知识</strong>：处理特定领域的知识和信息时可能遇到困难。</li>
<li><strong>透明性与可解释性</strong>：如何使模型的决策过程透明且易于解释。</li>
</ul>
</li>
<li>
<p><strong>对大语言模型（LLMs）的影响</strong></p>
<ul>
<li><strong>广泛影响</strong>：ICL在LLMs领域有着显著影响，许多机器人工作中使用ICL将LLMs应用于特定领域。</li>
<li><strong>图案识别能力</strong>：Mirchandani等人研究表明，LLMs通过ICL可以有效处理超出标准语言提示的通用模式。</li>
</ul>
</li>
<li>
<p><strong>应用场景</strong></p>
<ul>
<li><strong>离线轨迹优化</strong>：通过ICL优化机器人轨迹。</li>
<li><strong>在线ICL强化学习</strong>：在实时环境中应用ICL进行强化学习。</li>
</ul>
</li>
<li>
<p><strong>具体研究</strong></p>
<ul>
<li><strong>Jia等人的Chain-of-Thought Predictive Control</strong>
<ul>
<li><strong>方法</strong>：识别演示中的特定简短序列（称为“chain-of-thought”），理解和表示这些序列的层次结构。</li>
<li><strong>应用</strong>：特别关注接触丰富的物体操控任务中的子目标达成，探索从演示中学习机器人策略。</li>
</ul>
</li>
</ul>
</li>
</ol>
<h3>进一步解释</h3>
<ol>
<li>
<p><strong>In-context Learning (ICL)</strong></p>
<ul>
<li><strong>工作原理</strong>：通过提供示例来引导模型在不改变内部参数的情况下解决问题。</li>
<li><strong>应用广泛</strong>：在NLP任务中，如文本生成、问答等，利用提示中的示例来提高模型的表现。</li>
</ul>
</li>
<li>
<p><strong>Chain-of-Thought 方法</strong></p>
<ul>
<li><strong>逐步推理</strong>：解决复杂问题时，模型会通过多步推理过程生成最终答案。</li>
<li><strong>类人认知</strong>：模拟人类思维过程中的逐步推理，增强了模型的解释能力和透明性。</li>
</ul>
</li>
<li>
<p><strong>Mirchandani 等人的研究</strong></p>
<ul>
<li><strong>发现</strong>：LLMs通过ICL展示出卓越的模式识别能力，可以处理超出语言提示的通用模式。</li>
<li><strong>应用实例</strong>：例如，优化机器人轨迹和在线强化学习中的实际应用。</li>
</ul>
</li>
<li>
<p><strong>Jia等人的研究</strong></p>
<ul>
<li><strong>Chain-of-Thought Predictive Control</strong>：专注于从演示中提取和理解任务的层次结构和子目标。</li>
<li><strong>接触丰富的物体操控任务</strong>：研究如何从人类演示中学习，以提高机器人在实际任务中的表现。</li>
</ul>
</li>
</ol>
<p>通过这段解释，我们可以看到ICL如何通过提供示例而非参数优化来引导模型的学习过程，同时其在NLP和机器人领域的广泛应用及其面临的挑战和优势。</p>
<h2>E. robot transformers</h2>
<h3>解释和解读</h3>
<h4>Foundation Models的作用</h4>
<ol>
<li><strong>基础模型（Foundation Models）在机器人控制中的应用</strong>
<ul>
<li><strong>集成框架</strong>：基础模型提供了一个整合的框架，结合了感知、决策和动作生成三个方面，从而实现了机器人端到端的控制。这意味着机器人可以通过一个统一的系统来处理从环境感知、做出决策到执行动作的整个过程。</li>
<li><strong>优点</strong>：这种集成方法有助于简化系统设计，因为所有功能都集中在一个模型中，减少了模块之间的复杂接口和协调。同时，它提高了整体效率和性能，因为模型可以整体优化，而不仅仅是各个部分的优化。</li>
</ul>
</li>
</ol>
<h4>视觉预训练的应用</h4>
<ol start="2">
<li>
<p><strong>Xiao等人的研究</strong></p>
<ul>
<li><strong>自监督视觉预训练</strong>
<ul>
<li><strong>方法</strong>：使用真实世界的图像进行自监督的视觉预训练，直接从像素输入中学习运动控制任务。自监督学习是一种无需人工标注数据的学习方法，模型通过从未标记的数据中学习表示，从而掌握有用的特征。</li>
<li><strong>重点</strong>：研究主要集中在机器人操控任务上，这包括机器人如何移动物体、操作工具等。</li>
<li><strong>结果</strong>：预训练的视觉表示无需针对具体任务进行微调，就可以用于各种运动控制任务。这表明通过预训练获得的表示具有很强的泛化能力，可以适应不同的任务需求。</li>
<li><strong>潜力</strong>：展示了通过自监督学习从真实世界图像中获得通用视觉表示的潜力，这些表示可以应用于不同的运动控制任务。这意味着模型可以从大量的未标记图像中学习，并将这些知识应用到新的、未见过的任务中。</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Radosavovic等人的研究</strong></p>
<ul>
<li><strong>自监督视觉预训练</strong>
<ul>
<li><strong>方法</strong>：在各种野外视频上进行自监督的视觉预训练，用于现实世界中的机器人任务。野外视频通常包含丰富的视觉信息和动态变化的场景，非常适合用于训练具有广泛适应性的视觉模型。</li>
<li><strong>重点</strong>：研究同样集中在机器人操控任务上，这些任务可能涉及不同类型的机器人和操作环境。</li>
<li><strong>发现</strong>：从这些视频中获得的预训练表示在各种现实世界的机器人任务中都表现出色，并且适用于不同的机器人形式。这表明视觉表示不仅在一个特定任务上有效，而且在不同的任务和机器人平台上都能很好地泛化。</li>
<li><strong>结论</strong>：这些表示在各种任务和机器人平台上都能很好地泛化，展示了自监督预训练在现实世界机器人应用中的广泛适用性。自监督学习使得模型能够从多样化的数据中提取有用的信息，从而在不同的任务中表现出色。</li>
</ul>
</li>
</ul>
</li>
</ol>
<h4>视觉预训练的优势</h4>
<ol start="4">
<li><strong>自监督视觉预训练的优势</strong>
<ul>
<li><strong>大量无标签数据</strong>：模型通过大量的无标签数据进行训练，以学习有用的视觉表示。无标签数据的获取成本低，可以大规模收集，从而提供丰富的训练资源。</li>
<li><strong>多样且非结构化的数据</strong>：利用真实世界的图像和视频，这些方法能够从多样和非结构化的视觉数据中学习，产生更鲁棒和可迁移的表示。真实世界的数据包含各种复杂和多变的场景，这有助于训练出能够处理不同情况的模型。</li>
</ul>
</li>
</ol>
<h4>Transformer-based Policy Model的应用</h4>
<ol start="5">
<li><strong>Robotics Transformer (RT-1)</strong>
<ul>
<li><strong>模型特点</strong>：基于Transformer的策略模型，展示了良好的可扩展性。Transformer是一种强大的深度学习架构，最初用于自然语言处理，但已被证明在许多其他领域也非常有效。</li>
<li><strong>训练数据</strong>：模型的训练使用了超过130,000次的真实世界机器人经验数据，这些数据包含了700多个任务，由13台机器人在17个月内收集而成。如此庞大的数据集为模型提供了丰富的训练样本，使其能够学习到广泛的任务和场景。</li>
<li><strong>输入和输出</strong>：RT-1接收图像和自然语言指令作为输入，输出离散化的基础和臂部动作。这意味着模型不仅可以理解视觉信息，还能理解和执行基于语言的指令，具有很高的灵活性和适应性。</li>
<li><strong>泛化能力</strong>：能够泛化到新任务，在变化的环境中保持鲁棒性，并执行长远的指令。模型可以应对新的挑战和环境变化，显示出很强的适应能力。</li>
<li><strong>数据吸收能力</strong>：模型能够有效地吸收来自不同领域的数据，包括模拟和不同的机器人。这种多样化的数据来源使得模型能够处理更多种类的任务和场景。</li>
</ul>
</li>
</ol>
<h4>具体应用场景与优势</h4>
<ol start="6">
<li>
<p><strong>具体应用场景</strong></p>
<ul>
<li><strong>机器人操控任务</strong>：通过自监督视觉预训练，机器人能够从各种图像和视频数据中学习，应用于实际操控任务，如物体抓取和移动。这些任务需要机器人能够准确识别和操作不同的物体。</li>
<li><strong>任务执行与适应</strong>：RT-1展示了在执行复杂任务时的适应能力，能够处理来自不同环境和平台的数据。无论是在实验室环境还是在现实世界中，模型都能表现出色。</li>
</ul>
</li>
<li>
<p><strong>整体优势</strong></p>
<ul>
<li><strong>增强泛化能力</strong>：自监督学习和基础模型的结合大大提高了机器人在多样化任务中的表现。模型能够处理更多类型的任务和环境，提高了其实际应用的可行性。</li>
<li><strong>数据利用</strong>：通过利用大量的无标签数据和真实世界经验，模型可以获得更丰富的视觉表示，增强其适应性和鲁棒性。这使得机器人能够在各种复杂和动态的环境中有效工作。</li>
</ul>
</li>
</ol>
<h3>总结</h3>
<p>基础模型和自监督视觉预训练在机器人控制中的应用展示了其巨大的潜力和优势。通过结合感知、决策和动作生成，这些方法不仅提高了系统的集成性和效率，还增强了模型在多样化任务中的泛化能力和适应性。基于Transformer的策略模型RT-1则进一步展示了在复杂任务执行和数据吸收能力方面的卓越表现。这些技术的应用不仅推动了机器人技术的发展，也为实现更智能、更自主的机器人系统奠定了基础。</p>
<h3>解释和解读</h3>
<h4>Robotic Transformer 2 (RT-2)</h4>
<ol>
<li>
<p><strong>RT-2模型的创新</strong></p>
<ul>
<li><strong>视觉-语言-动作（VLA）模型</strong>：RT-2模型在RT-1的基础上进一步发展，通过结合网络数据和机器人数据来学习。RT-2不仅能处理视觉和语言信息，还能生成用于机器人控制的通用动作。</li>
<li><strong>数据来源</strong>：模型使用了现有的视觉-语言模型，并直接在机器人轨迹上进行共同微调（co-fine-tuning）。这种方法使得单一模型可以同时作为语言模型、视觉-语言模型和机器人策略模型来工作。</li>
</ul>
</li>
<li>
<p><strong>共同微调（co-fine-tuning）</strong></p>
<ul>
<li><strong>动作表示</strong>：为了实现共同微调，动作被表示为简单的文本字符串，然后使用大型语言模型（LLM）的分词器将其转换为文本令牌（tokens）。这使得模型可以处理低级闭环控制，即根据摄像头观察到的环境和指令生成机器人动作。</li>
<li><strong>动作空间</strong>：包括机器人末端执行器的6自由度（6-DoF）位置和旋转位移、抓取器的扩展以及任务终止命令。这样的设计允许机器人执行复杂的操作和移动。</li>
</ul>
</li>
<li>
<p><strong>实验结果</strong></p>
<ul>
<li><strong>广泛的实验</strong>：通过大量实验，作者展示了使用视觉-语言模型（VLMs）有助于增强在视觉和语义概念上的泛化能力，使机器人能够响应所谓的链式思维提示（chain of thought prompting）。这种提示方式让代理可以进行更复杂的多阶段语义推理。</li>
<li><strong>任务类型</strong>：RT-1和RT-2都考虑了机器人操控和导航任务，使用的是Everyday Robots的现实世界移动操控机器人。</li>
</ul>
</li>
</ol>
<h4>RT-2的局限性与未来研究方向</h4>
<ul>
<li><strong>局限性</strong>：当前RT-2模型的主要限制在于其技能范围仅限于训练数据中的技能分布。即，机器人只能执行在其训练数据中出现过的操作和任务。</li>
<li><strong>未来方向</strong>：
<ul>
<li><strong>多样化数据收集</strong>：通过收集更广泛和多样化的机器人操作数据，可以扩展机器人的技能范围。例如，记录更多类型的任务和操作视频。</li>
<li><strong>使用人类视频数据</strong>：从人类执行任务的视频中学习动作，可以帮助机器人掌握更自然和灵活的操作技能。</li>
<li><strong>机器人模拟</strong>：利用虚拟环境中的模拟数据进行训练，这可以生成大量的训练样本，并在真实世界中进行测试和验证。</li>
<li><strong>不同机器人形式的数据</strong>：研究不同类型和形态的机器人数据，这有助于提高模型的泛化能力，使其能够适应各种机器人平台。</li>
</ul>
</li>
</ul>
<h3>总结</h3>
<p>RT-2模型通过结合网络数据和机器人数据，实现了视觉-语言-动作的集成控制，展示了在机器人操作和导航任务中的强大能力。尽管其技能范围受限于训练数据，但通过进一步多样化数据来源和使用先进的训练方法，RT-2模型有望在未来实现更广泛的应用和更高的灵活性。这一系列研究不仅推进了机器人技术的发展，也为实现更智能和自主的机器人系统奠定了坚实基础。</p>
<h3>解释和解读</h3>
<h4>RT-X模型的多机器人架构学习</h4>
<ol>
<li>
<p><strong>RT-X模型的创新</strong></p>
<ul>
<li><strong>多机器人化学习</strong>：RT-X利用了来自不同机器人形式的数据，通过将数据标准化和模型统一化，使得跨机器人平台的大规模训练成为可能。这意味着模型可以在多个不同类型的机器人上进行训练和应用，扩展了机器人的操作能力和适应性。</li>
<li><strong>数据集</strong>：RT-X数据集由22个不同机器人收集而成，涉及21个机构，共包含527种技能（160266个任务）。这是一项大规模的合作，旨在提供一个统一的、多样化的数据集，用于机器人操控任务的研究。</li>
</ul>
</li>
<li>
<p><strong>跨领域的正迁移</strong></p>
<ul>
<li><strong>正迁移</strong>：通过在这种多机器人、多任务的数据上进行训练，RT-1和RT-2模型展示了跨机器人领域的正迁移效果。这意味着一个机器人在特定任务上的经验可以转移到其他机器人上，提升其在不同任务中的表现。</li>
<li><strong>能力提升</strong>：多个机器人通过利用其他平台的经验，其能力得到了显著提升。这种方法展示了在多种机器人平台之间共享和传递知识的潜力。</li>
</ul>
</li>
</ol>
<h4>Perception-Action Causal Transformer (PACT)</h4>
<ol start="3">
<li>
<p><strong>PACT模型的特点</strong></p>
<ul>
<li><strong>生成式Transformer架构</strong>：PACT是一种生成式的Transformer架构，通过自监督学习从机器人数据中构建表示。自监督学习使模型能够从大量未标记的轨迹数据中学习有效的表示。</li>
<li><strong>机器人导航任务</strong>：研究主要集中在机器人导航任务上，模型通过预训练来学习适用于多任务的表示。</li>
</ul>
</li>
<li>
<p><strong>训练与预测</strong></p>
<ul>
<li><strong>训练数据</strong>：PACT使用来自机器人的丰富的安全状态-动作数据（轨迹）进行训练，学习预测合适的安全动作。模型通过自回归方式预测随时间变化的状态和动作，从而隐式地捕捉到机器人特定的动态和行为。</li>
<li><strong>实验结果</strong>：PACT在轮式机器人（MuSHR）和模拟代理（使用第一人称RGB图像的Habitat）上进行了测试，展示了其在安全导航、定位和建图任务中的有效性。实验还表明，在预训练模型上微调特定任务的小型网络，<strong>性能明显优于从头开始同时训练所有任务的单一模型，并且与为每个任务独立训练的大型模型性能相当。</strong></li>
</ul>
</li>
</ol>
<h4>Self-supervised Multi-task pretrAining with contRol Transformer (SMART)</h4>
<ol start="5">
<li>
<p><strong>SMART模型的特点</strong></p>
<ul>
<li><strong>自监督多任务预训练</strong>：SMART引入了一种自监督的多任务预训练方法，专为控制Transformer设计，旨在捕捉短期和长期控制所需的信息。这种预训练-微调的方法能够跨各种任务进行知识转移，提高学习效率。</li>
<li><strong>任务范围</strong>：SMART模型适用于多个任务，包括推杆摆动、推杆平衡、跳跃、奔跑、行走等。这些任务涵盖了多种领域，展示了模型的广泛适用性。</li>
</ul>
</li>
<li>
<p><strong>实验结果</strong></p>
<ul>
<li><strong>对分布变化的鲁棒性</strong>：SMART展示了对分布变化的鲁棒性，即在面对数据分布的变化时仍能保持良好的性能。此外，实验还表明，即使使用质量较低的预训练数据集，SMART仍能有效工作。</li>
</ul>
</li>
</ol>
<h3>详细讲解</h3>
<h4>多机器人化学习的潜力</h4>
<ul>
<li><strong>跨平台的经验共享</strong>：通过在多个机器人平台上训练，RT-X模型展示了跨平台经验共享的潜力。一个机器人在特定任务中的经验可以用于提升其他机器人的表现，这种正迁移效果有助于提高整体系统的效率和鲁棒性。</li>
<li><strong>统一数据格式</strong>：标准化的数据格式和统一的模型结构使得跨平台训练成为可能。不同机构和平台的数据被整合在一起，形成一个统一的大规模数据集，为研究提供了丰富的资源。</li>
</ul>
<h4>PACT模型的生成式学习</h4>
<ul>
<li><strong>生成式Transformer</strong>：生成式Transformer模型通过预测随时间变化的状态和动作，捕捉到机器人特定的动态和行为。这种方法不仅适用于导航任务，还可以扩展到其他类型的任务，如操控和交互。</li>
<li><strong>自监督学习</strong>：通过自监督学习，模型可以从未标记的轨迹数据中学习有效的表示。这种学习方法具有较高的通用性，可以应用于不同的任务和场景。</li>
</ul>
<h4>SMART模型的多任务预训练</h4>
<ul>
<li><strong>多任务预训练</strong>：SMART通过自监督的多任务预训练方法，在短期和长期控制任务中均表现出色。预训练阶段捕捉到的有用信息有助于后续任务的微调，提高了模型的学习效率和性能。</li>
<li><strong>广泛适用性</strong>：SMART适用于多种任务和领域，展示了其广泛的适用性。无论是简单的平衡任务还是复杂的奔跑任务，模型都能有效应对。</li>
</ul>
<h3>总结</h3>
<p>通过RT-X、PACT和SMART模型的研究，展示了Transformer架构在机器人控制中的巨大潜力。多机器人化学习、自监督生成式学习和多任务预训练等方法不仅提高了模型的性能和泛化能力，还展示了在多样化任务中的广泛适用性。这些研究成果为实现更智能、更自主的机器人系统提供了重要的技术支持，同时也为未来的研究指明了方向。</p>
<h5>1. 模块化机器人控制架构</h5>
<ul>
<li><strong>定义</strong>：模块化机器人控制架构是将机器人控制任务分解为若干独立的模块，每个模块专注于特定的任务或功能。通过模块化设计，可以更灵活地应对复杂的机器人控制问题，提高系统的鲁棒性和扩展性。</li>
<li><strong>组成部分</strong>：通常包括感知（Perception）、规划（Planning）、控制（Control）等多个模块。各模块之间通过定义良好的接口进行通信和协作。</li>
</ul>
<h5>2. LATTE模型的介绍</h5>
<ul>
<li><strong>LATTE模型</strong>：LATTE（Language And Trajectory Transformer Engine）是一种多模态Transformer模型，旨在通过语言指令重新调整机器人轨迹。该模型结合了经典的规划和控制层次，适用于机器人操作和导航任务。</li>
</ul>
<h5>3. LATTE模型的输入和输出</h5>
<ul>
<li>
<p><strong>输入</strong>：</p>
<ul>
<li><strong>几何特征</strong>：初始轨迹的几何特征，如路径点的位置和方向。</li>
<li><strong>障碍物地图配置</strong>：环境中障碍物的分布情况，用于规划避障路径。</li>
<li><strong>用户语言指令</strong>：用户通过自然语言描述的目标和要求。</li>
<li><strong>环境中每个对象的图像</strong>：用于识别和定位环境中的物体。</li>
</ul>
</li>
<li>
<p><strong>输出</strong>：</p>
<ul>
<li><strong>修改后的轨迹点</strong>：模型根据用户的语言指令调整每个路径点，使最终的机器人运动符合用户的期望。</li>
</ul>
</li>
</ul>
<h5>4. 初始轨迹规划</h5>
<ul>
<li><strong>几何规划器</strong>：初始轨迹可以通过几何规划器生成，例如A*、RRT*或模型预测控制（MPC）。这些规划器根据环境地图和目标位置生成一条初始路径。
<ul>
<li><strong>A</strong>*：一种经典的路径搜索算法，能够找到从起点到终点的最短路径。</li>
<li><strong>RRT</strong>*：快速扩展随机树（RRT）算法的改进版本，用于生成无碰撞路径，适合高维空间中的路径规划。</li>
<li><strong>模型预测控制（MPC）</strong>：通过预测未来的系统状态来优化控制输入，使得系统能够跟踪期望的轨迹。</li>
</ul>
</li>
</ul>
<h5>5. 语义目标的整合</h5>
<ul>
<li><strong>语义目标整合</strong>：初始路径生成后，LATTE模型结合预训练的语言和视觉-语言模型，利用世界的语义表示丰富路径的语义目标。这使得机器人能够理解和执行复杂的语言指令，生成符合用户期望的路径。</li>
</ul>
<h5>6. 预训练模型的应用</h5>
<ul>
<li><strong>预训练语言模型</strong>：利用大规模文本数据进行预训练的语言模型，能够理解和生成自然语言。例如，GPT、BERT等。</li>
<li><strong>视觉-语言模型</strong>：结合图像和文本数据进行预训练的模型，能够在视觉和语言之间建立关联。例如，CLIP、ViLT等。</li>
</ul>
<h5>7. 实现流程</h5>
<ol>
<li><strong>数据收集</strong>：收集机器人轨迹数据、环境地图、物体图像和用户语言指令。</li>
<li><strong>初始轨迹规划</strong>：使用几何规划器生成初始轨迹，考虑环境中的障碍物配置。</li>
<li><strong>模型输入</strong>：将初始轨迹的几何特征、障碍物地图、用户语言指令和物体图像输入LATTE模型。</li>
<li><strong>轨迹调整</strong>：LATTE模型根据输入信息，调整轨迹点，使最终路径符合用户指令。</li>
<li><strong>输出轨迹</strong>：生成修改后的轨迹，机器人按照调整后的路径进行操作和导航。</li>
</ol>
<h5>8. LATTE模型的创新</h5>
<ul>
<li><strong>多模态输入</strong>：LATTE模型结合几何特征、障碍物地图、语言指令和物体图像，提供了一个多模态输入的框架，能够更好地理解和执行复杂任务。</li>
<li><strong>预训练模型的应用</strong>：利用预训练的语言和视觉-语言模型，LATTE模型能够捕捉世界的语义表示，提高机器人对语言指令的理解和执行能力。</li>
<li><strong>语义目标整合</strong>：通过结合几何规划和语义目标，LATTE模型能够生成符合用户期望的路径，展示了语义信息在机器人控制中的重要性。</li>
</ul>
<h4>补充知识</h4>
<ol>
<li><strong>经典路径规划算法</strong>
<ul>
<li><strong>A</strong>*：一种广泛使用的图搜索算法，通过启发式函数估计路径代价，从起点到终点找到代价最小的路径。适用于静态环境中的路径规划。</li>
<li><strong>RRT</strong>*：快速扩展随机树算法的改进版本，能够高效地在复杂环境中生成无碰撞路径，适用于高维空间中的路径规划。</li>
<li><strong>模型预测控制（MPC）</strong>：通过预测未来的系统状态和优化控制输入，使系统能够跟踪期望的轨迹，广泛应用于机器人控制和自动驾驶。</li>
</ul>
</li>
</ol>
    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2024/11/19/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80-%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E7%AF%87/" rel="prev" title="">
      <i class="fa fa-chevron-left"></i> 
    </a></div>
      <div class="post-nav-item"></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link"><span class="nav-number">1.</span> <span class="nav-text">预训练基础模型在机器人技术中的应用</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">1.1.</span> <span class="nav-text">基础模型背景</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">1.1.1.</span> <span class="nav-text">Tokenization 过程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">1.1.2.</span> <span class="nav-text">LLM 和 GPT-3 的应用</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">1.1.3.</span> <span class="nav-text">总结</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">1.1.4.</span> <span class="nav-text">1. 标记解码过程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">1.1.5.</span> <span class="nav-text">2. 利用权重引入随机性</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">1.1.6.</span> <span class="nav-text">3. 随机性来源的说明</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">1.1.7.</span> <span class="nav-text">字节对编码（BPE）的工作原理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">1.1.8.</span> <span class="nav-text">一、初始化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">1.1.9.</span> <span class="nav-text">二、迭代合并</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">1.1.10.</span> <span class="nav-text">三、举例</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">1.1.11.</span> <span class="nav-text">四、编码过程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">1.1.12.</span> <span class="nav-text">五、总结</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">1.1.13.</span> <span class="nav-text">BPE在GPT系列模型中的应用</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">1.1.14.</span> <span class="nav-text">BPE在其他领域的应用</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link"><span class="nav-number">2.</span> <span class="nav-text">Transformer模型中的注意力头机制</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">2.1.</span> <span class="nav-text">1. 线性变换</span></a></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">2.2.</span> <span class="nav-text">2. 缩放点积注意力</span></a></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">2.3.</span> <span class="nav-text">3. 加权求和</span></a></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">2.4.</span> <span class="nav-text">4. 矩阵形式的计算</span></a></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">2.5.</span> <span class="nav-text">5. 多头注意力</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">2.5.1.</span> <span class="nav-text">掩码自编码（Masked Auto-Encoding）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">2.5.2.</span> <span class="nav-text">对比学习（Contrastive Learning）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">2.5.3.</span> <span class="nav-text">一、前向过程（Forward Process）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">2.5.4.</span> <span class="nav-text">二、反向过程（Reverse Process）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">2.5.5.</span> <span class="nav-text">三、损失函数（Loss Function）</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">2.6.</span> <span class="nav-text">B.大型语言模型(LLM)示例和历史</span></a></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">2.7.</span> <span class="nav-text">Vision Transformers（ViT）概述：</span></a></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">2.8.</span> <span class="nav-text">多模态视觉语言模型（VLMs）概述：</span></a></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">2.9.</span> <span class="nav-text">E. Embodied Multimodal Language Models</span></a></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">2.10.</span> <span class="nav-text">F. 视觉生成模型：</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link"><span class="nav-number">3.</span> <span class="nav-text">机器人技术</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">3.1.</span> <span class="nav-text">机器人决策控制与策略学习</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">3.1.1.</span> <span class="nav-text">语言条件模仿学习概述</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">3.1.2.</span> <span class="nav-text">数学基础</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">3.1.3.</span> <span class="nav-text">原理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">3.1.4.</span> <span class="nav-text">内容</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">3.1.5.</span> <span class="nav-text">在机器人操控任务中的应用</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">3.1.6.</span> <span class="nav-text">总结</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">3.1.7.</span> <span class="nav-text">多上下文模仿学习</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">3.1.8.</span> <span class="nav-text">应对数据标注挑战</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">3.1.9.</span> <span class="nav-text">CLIPort</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">3.1.10.</span> <span class="nav-text">PerAct（Perceiver-Actor）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">3.1.11.</span> <span class="nav-text">总结</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">3.1.12.</span> <span class="nav-text">核心思想</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">3.1.13.</span> <span class="nav-text">应用任务</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">3.1.14.</span> <span class="nav-text">挑战与解决方案</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">3.1.15.</span> <span class="nav-text">1. 数据收集阶段</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">3.1.16.</span> <span class="nav-text">2. 数据增强阶段</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">3.1.17.</span> <span class="nav-text">3. 视觉表征学习阶段</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">3.1.18.</span> <span class="nav-text">4. 模仿策略训练阶段</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">3.1.19.</span> <span class="nav-text">总结</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">3.1.20.</span> <span class="nav-text">论文解读：任务规范和语言辅助强化学习的最新进展</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-number">3.1.20.1.</span> <span class="nav-text">一、任务规范的多样化探索</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-number">3.1.20.2.</span> <span class="nav-text">二、语言辅助的强化学习</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">3.1.21.</span> <span class="nav-text">关键点总结</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">3.1.22.</span> <span class="nav-text">语言-图像目标条件值学习的详细讲解</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-number">3.1.22.1.</span> <span class="nav-text">1. R3M (Reusable Representation for Robotic Manipulations)</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">3.1.23.</span> <span class="nav-text">详细讲解 Value-Implicit Pretraining (VIP)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-number">3.1.23.1.</span> <span class="nav-text">概述</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-number">3.1.23.2.</span> <span class="nav-text">实现细节</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link"><span class="nav-number">3.1.23.2.1.</span> <span class="nav-text">1. 数据预处理与预训练</span></a></li><li class="nav-item nav-level-5"><a class="nav-link"><span class="nav-number">3.1.23.2.2.</span> <span class="nav-text">2. 时间对比学习目标</span></a></li><li class="nav-item nav-level-5"><a class="nav-link"><span class="nav-number">3.1.23.2.3.</span> <span class="nav-text">3. 视觉目标条件值函数学习</span></a></li><li class="nav-item nav-level-5"><a class="nav-link"><span class="nav-number">3.1.23.2.4.</span> <span class="nav-text">4. 零样本奖励指定</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">3.1.24.</span> <span class="nav-text">关键实现步骤</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">3.1.25.</span> <span class="nav-text">应用实例</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">3.1.26.</span> <span class="nav-text">关键点总结</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">3.1.27.</span> <span class="nav-text">语言-图像值学习 (LIV) 详细讲解</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-number">3.1.27.1.</span> <span class="nav-text">概述</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-number">3.1.27.2.</span> <span class="nav-text">实现细节</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link"><span class="nav-number">3.1.27.2.1.</span> <span class="nav-text">1. 数据预处理与预训练</span></a></li><li class="nav-item nav-level-5"><a class="nav-link"><span class="nav-number">3.1.27.2.2.</span> <span class="nav-text">2. 预训练模型结构</span></a></li><li class="nav-item nav-level-5"><a class="nav-link"><span class="nav-number">3.1.27.2.3.</span> <span class="nav-text">3. 值函数学习</span></a></li><li class="nav-item nav-level-5"><a class="nav-link"><span class="nav-number">3.1.27.2.4.</span> <span class="nav-text">4. 策略学习</span></a></li><li class="nav-item nav-level-5"><a class="nav-link"><span class="nav-number">3.1.27.2.5.</span> <span class="nav-text">5. 微调与应用</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">3.1.28.</span> <span class="nav-text">关键实现步骤</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">3.1.29.</span> <span class="nav-text">应用实例</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-number">3.1.29.1.</span> <span class="nav-text">其他相关方法和应用</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">3.1.30.</span> <span class="nav-text">VoxPoser [34]</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">3.1.31.</span> <span class="nav-text">Reward Shaping Using CLIP</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">3.1.32.</span> <span class="nav-text">Hierarchical Universal Language Conditioned Policies 2.0 (HULC++)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">3.1.33.</span> <span class="nav-text">关键点总结</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">3.1.34.</span> <span class="nav-text">Robot Task Planning using Large Language Models</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-number">3.1.34.1.</span> <span class="nav-text">1. 任务规范的语言指令</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-number">3.1.34.2.</span> <span class="nav-text">2. 使用语言模型生成任务规划的代码</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">3.1.35.</span> <span class="nav-text">Code-as-Policies</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-number">3.1.35.1.</span> <span class="nav-text">主要思想和实现细节：</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">3.2.</span> <span class="nav-text">D.上下文学习对决策的影响</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">3.2.1.</span> <span class="nav-text">In-context Learning (ICL)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">3.2.2.</span> <span class="nav-text">进一步解释</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">3.3.</span> <span class="nav-text">E. robot transformers</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">3.3.1.</span> <span class="nav-text">解释和解读</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-number">3.3.1.1.</span> <span class="nav-text">Foundation Models的作用</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-number">3.3.1.2.</span> <span class="nav-text">视觉预训练的应用</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-number">3.3.1.3.</span> <span class="nav-text">视觉预训练的优势</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-number">3.3.1.4.</span> <span class="nav-text">Transformer-based Policy Model的应用</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-number">3.3.1.5.</span> <span class="nav-text">具体应用场景与优势</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">3.3.2.</span> <span class="nav-text">总结</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">3.3.3.</span> <span class="nav-text">解释和解读</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-number">3.3.3.1.</span> <span class="nav-text">Robotic Transformer 2 (RT-2)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-number">3.3.3.2.</span> <span class="nav-text">RT-2的局限性与未来研究方向</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">3.3.4.</span> <span class="nav-text">总结</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">3.3.5.</span> <span class="nav-text">解释和解读</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-number">3.3.5.1.</span> <span class="nav-text">RT-X模型的多机器人架构学习</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-number">3.3.5.2.</span> <span class="nav-text">Perception-Action Causal Transformer (PACT)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-number">3.3.5.3.</span> <span class="nav-text">Self-supervised Multi-task pretrAining with contRol Transformer (SMART)</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">3.3.6.</span> <span class="nav-text">详细讲解</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-number">3.3.6.1.</span> <span class="nav-text">多机器人化学习的潜力</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-number">3.3.6.2.</span> <span class="nav-text">PACT模型的生成式学习</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-number">3.3.6.3.</span> <span class="nav-text">SMART模型的多任务预训练</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">3.3.7.</span> <span class="nav-text">总结</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link"><span class="nav-number">3.3.7.0.1.</span> <span class="nav-text">1. 模块化机器人控制架构</span></a></li><li class="nav-item nav-level-5"><a class="nav-link"><span class="nav-number">3.3.7.0.2.</span> <span class="nav-text">2. LATTE模型的介绍</span></a></li><li class="nav-item nav-level-5"><a class="nav-link"><span class="nav-number">3.3.7.0.3.</span> <span class="nav-text">3. LATTE模型的输入和输出</span></a></li><li class="nav-item nav-level-5"><a class="nav-link"><span class="nav-number">3.3.7.0.4.</span> <span class="nav-text">4. 初始轨迹规划</span></a></li><li class="nav-item nav-level-5"><a class="nav-link"><span class="nav-number">3.3.7.0.5.</span> <span class="nav-text">5. 语义目标的整合</span></a></li><li class="nav-item nav-level-5"><a class="nav-link"><span class="nav-number">3.3.7.0.6.</span> <span class="nav-text">6. 预训练模型的应用</span></a></li><li class="nav-item nav-level-5"><a class="nav-link"><span class="nav-number">3.3.7.0.7.</span> <span class="nav-text">7. 实现流程</span></a></li><li class="nav-item nav-level-5"><a class="nav-link"><span class="nav-number">3.3.7.0.8.</span> <span class="nav-text">8. LATTE模型的创新</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-number">3.3.7.1.</span> <span class="nav-text">补充知识</span></a></li></ol></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">John Doe</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">3</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">John Doe</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
